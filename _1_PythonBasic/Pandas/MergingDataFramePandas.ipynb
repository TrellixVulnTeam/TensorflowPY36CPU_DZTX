{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Reading DataFrames from multiple files\n",
    "\n",
    "When data is spread among several files, you usually invoke pandas' read_csv() (or a similar data import function) multiple times to load the data into several DataFrames.\n",
    "\n",
    "The data files for this example have been derived from a list of Olympic medals awarded between 1896 & 2008 compiled by the Guardian.\n",
    "\n",
    "The column labels of each DataFrame are NOC, Country, & Total where NOC is a three-letter code for the name of the country and Total is the number of medals of that type won (bronze, silver, or gold).\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   NOC         Country   Total\n0  USA   United States  2088.0\n1  URS    Soviet Union   838.0\n2  GBR  United Kingdom   498.0\n3  FRA          France   378.0\n4  GER         Germany   407.0\n"
     ]
    }
   ],
   "source": [
    "# Import pandas\n",
    "import pandas as pd\n",
    "\n",
    "path = '/Users/victor/Documents/Dev/TensorflowPY36CPU/_1_PythonBasic/Pandas/'\n",
    "\n",
    "# Read 'Bronze.csv' into a DataFrame: bronze\n",
    "bronze = pd.read_csv(path+'Bronze.csv')\n",
    "\n",
    "# Read 'Silver.csv' into a DataFrame: silver\n",
    "silver = pd.read_csv(path+'Silver.csv')\n",
    "\n",
    "# Read 'Gold.csv' into a DataFrame: gold\n",
    "gold = pd.read_csv(path+'Gold.csv')\n",
    "\n",
    "# Print the first five rows of gold\n",
    "print(gold.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading DataFrames from multiple files in a loop\n",
    "As you saw in the video, loading data from multiple files into DataFrames is more efficient in a loop or a list comprehension.\n",
    "\n",
    "Notice that this approach is not restricted to working with CSV files. That is, even if your data comes in other formats, as long as pandas has a suitable data import function, you can apply a loop or comprehension to generate a list of DataFrames imported from the source files.\n",
    "\n",
    "Here, you'll continue working with The Guardian's Olympic medal dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   NOC         Country   Total\n0  USA   United States  2088.0\n1  URS    Soviet Union   838.0\n2  GBR  United Kingdom   498.0\n3  FRA          France   378.0\n4  GER         Germany   407.0\n"
     ]
    }
   ],
   "source": [
    "# Create the list of file names: filenames\n",
    "filenames = [path+'Gold.csv', path+'Silver.csv', path+'Bronze.csv']\n",
    "\n",
    "# Create the list of three DataFrames: dataframes\n",
    "dataframes = []\n",
    "for filename in filenames:\n",
    "    dataframes.append(pd.read_csv(filename))\n",
    "\n",
    "# Print top 5 rows of 1st DataFrame in dataframes\n",
    "print(dataframes[0].head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Combining DataFrames from multiple data files\n",
    "In this exercise, you'll combine the three DataFrames from earlier exercises - gold, silver, & bronze - into a single DataFrame called medals. The approach you'll use here is clumsy. Later on in the course, you'll see various powerful methods that are frequently used in practice for concatenating or merging DataFrames.\n",
    "\n",
    "Remember, the column labels of each DataFrame are NOC, Country, and Total, where NOC is a three-letter code for the name of the country and Total is the number of medals of that type won."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   NOC         Country    Gold  Silver  Bronze\n0  USA   United States  2088.0  1195.0  1052.0\n1  URS    Soviet Union   838.0   627.0   584.0\n2  GBR  United Kingdom   498.0   591.0   505.0\n3  FRA          France   378.0   461.0   475.0\n4  GER         Germany   407.0   350.0   454.0\n"
     ]
    }
   ],
   "source": [
    "# Make a copy of gold: medals\n",
    "medals = gold.copy()\n",
    "\n",
    "# Create list of new column labels: new_labels\n",
    "new_labels = ['NOC', 'Country', 'Gold']\n",
    "\n",
    "# Rename the columns of medals using new_labels\n",
    "medals.columns = new_labels\n",
    "\n",
    "# Add columns 'Silver' & 'Bronze' to medals\n",
    "medals['Silver'] = silver['Total'] \n",
    "medals['Bronze'] = bronze['Total']\n",
    "\n",
    "# Print the head of medals\n",
    "print(medals.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## using Listing Comprehehension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataframe2 = pd.concat([pd.read_csv(f) for f in filenames])\n",
    "dataframe2.equals(dataframes)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# using Glob modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "from glob import glob\n",
    "filesnames = glob('sales*.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/Users/victor/Documents/Dev/TensorflowPY36CPU/_1_PythonBasic/Pandas/Gold.csv',\n '/Users/victor/Documents/Dev/TensorflowPY36CPU/_1_PythonBasic/Pandas/Silver.csv',\n '/Users/victor/Documents/Dev/TensorflowPY36CPU/_1_PythonBasic/Pandas/Bronze.csv']"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filenames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "globadf = pd.concat([pd.read_csv(f) for f in filenames]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pandas.core.frame.DataFrame"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(globadf)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sorting DataFrame with the Index & columns\n",
    "\n",
    "It is often useful to rearrange the sequence of the rows of a DataFrame by sorting. You don't have to implement these yourself; the principal methods for doing this are .sort_index() and .sort_values().\n",
    "\n",
    "In this exercise, you'll use these methods with a DataFrame of temperature values indexed by month names. You'll sort the rows alphabetically using the Index and numerically using a column. Notice, for this data, the original ordering is probably most useful and intuitive: the purpose here is for you to understand what the sorting methods do.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        Max TemperatureF\nMonth                   \nJan                   68\nFeb                   60\nMar                   68\nApr                   84\nMay                   88\n           Max TemperatureF\nMonth                      \nApr                      84\nAug                      86\nDec                      68\nFeb                      60\nJan                      68\n             Max TemperatureF\nMonth                        \nSep                        90\nOct                        84\nNov                        72\nMay                        88\nMar                        68\n       Max TemperatureF\nMonth                  \nFeb                  60\nJan                  68\nMar                  68\nDec                  68\nNov                  72\n"
     ]
    }
   ],
   "source": [
    "# Import pandas\n",
    "import pandas as pd\n",
    "\n",
    "# Read 'monthly_max_temp.csv' into a DataFrame: weather1\n",
    "weather1 = pd.read_csv(path+'maxtemperture.csv',index_col='Month')\n",
    "\n",
    "# Print the head of weather1\n",
    "print(weather1.head())\n",
    "\n",
    "# Sort the index of weather1 in alphabetical order: weather2\n",
    "weather2 = weather1.sort_index()\n",
    "\n",
    "# Print the head of weather2\n",
    "print(weather2.head())\n",
    "\n",
    "# Sort the index of weather1 in reverse alphabetical order: weather3\n",
    "weather3 = weather1.sort_index(ascending=False)\n",
    "\n",
    "# Print the head of weather3\n",
    "print(weather3.head())\n",
    "\n",
    "# Sort weather1 numerically using the values of 'Max TemperatureF': weather4\n",
    "weather4 =weather1.sort_values('Max TemperatureF')\n",
    "\n",
    "# Print the head of weather4\n",
    "print(weather4.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## rindexing DataFrame from a list\n",
    "Sorting methods are not the only way to change DataFrame Indexes. There is also the .reindex() method.\n",
    "\n",
    "In this exercise, you'll reindex a DataFrame of quarterly-sampled mean temperature values to contain monthly samples (this is an example of upsampling or increasing the rate of samples, which you may recall from the pandas Foundations course).\n",
    "\n",
    "The original data has the first month's abbreviation of the quarter (three-month interval) on the Index, namely Apr, Jan, Jul, and Sep. This data has been loaded into a DataFrame called weather1 and has been printed in its entirety in the IPython Shell. Notice it has only four rows (corresponding to the first month of each quarter) and that the rows are not sorted chronologically.\n",
    "\n",
    "You'll initially use a list of all twelve month abbreviations and subsequently apply the .ffill() method to forward-fill the null entries when upsampling. This list of month abbreviations has been pre-loaded as year."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "File b'K:\\\\TensorflowPY36CPU\\\\TensorflowPY36CPU\\\\_1_PythonBasic\\\\Pandas\\\\temperture.csv' does not exist",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-89-5fd42cf116a0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mweather1\u001b[0m  \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'K:\\\\TensorflowPY36CPU\\\\TensorflowPY36CPU\\\\_1_PythonBasic\\\\Pandas\\\\temperture.csv'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mindex_col\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'Month'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/TensorflowCPUPY35/lib/python3.5/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36mparser_f\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, escapechar, comment, encoding, dialect, tupleize_cols, error_bad_lines, warn_bad_lines, skipfooter, skip_footer, doublequote, delim_whitespace, as_recarray, compact_ints, use_unsigned, low_memory, buffer_lines, memory_map, float_precision)\u001b[0m\n\u001b[1;32m    707\u001b[0m                     skip_blank_lines=skip_blank_lines)\n\u001b[1;32m    708\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 709\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    710\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    711\u001b[0m     \u001b[0mparser_f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/TensorflowCPUPY35/lib/python3.5/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    447\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    448\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 449\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    450\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    451\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/TensorflowCPUPY35/lib/python3.5/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m    816\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'has_index_names'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'has_index_names'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    817\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 818\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    819\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    820\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/TensorflowCPUPY35/lib/python3.5/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, engine)\u001b[0m\n\u001b[1;32m   1047\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'c'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1048\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'c'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1049\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCParserWrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1050\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1051\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'python'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/TensorflowCPUPY35/lib/python3.5/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, src, **kwds)\u001b[0m\n\u001b[1;32m   1693\u001b[0m         \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'allow_leading_cols'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex_col\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1694\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1695\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparsers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTextReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1696\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1697\u001b[0m         \u001b[0;31m# XXX\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.__cinit__\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._setup_parser_source\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: File b'K:\\\\TensorflowPY36CPU\\\\TensorflowPY36CPU\\\\_1_PythonBasic\\\\Pandas\\\\temperture.csv' does not exist"
     ],
     "output_type": "error"
    }
   ],
   "source": [
    "# Import pandas\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "weather1  = pd.read_csv('K:\\\\TensorflowPY36CPU\\\\TensorflowPY36CPU\\\\_1_PythonBasic\\\\Pandas\\\\temperture.csv',index_col='Month')\n",
    "\n",
    "\n",
    "\n",
    "year = ['Jan',\n",
    "        'Feb',\n",
    "        'Mar',\n",
    "        'Apr',\n",
    "        'May',\n",
    "        'Jun',\n",
    "        'Jul',\n",
    "        'Aug',\n",
    "        'Sep',\n",
    "        'Oct',\n",
    "        'Nov',\n",
    "        'Dec']\n",
    "\n",
    "\n",
    "\n",
    "# Reindex weather1 using the list year: weather2\n",
    "weather2 = weather1.reindex(year)\n",
    "\n",
    "# Print weather2\n",
    "print(weather2)\n",
    "\n",
    "# Reindex weather1 using the list year with forward-fill: weather3\n",
    "weather3 = weather1.reindex(year).ffill()\n",
    "\n",
    "# Print weather3\n",
    "print(weather3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Max TemperatureF</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Month</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Jan</th>\n",
       "      <td>68</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Feb</th>\n",
       "      <td>60</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Mar</th>\n",
       "      <td>68</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Apr</th>\n",
       "      <td>84</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>May</th>\n",
       "      <td>88</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Max TemperatureF</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Month</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Jan</th>\n",
       "      <td>68</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Feb</th>\n",
       "      <td>60</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Mar</th>\n",
       "      <td>68</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Apr</th>\n",
       "      <td>84</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>May</th>\n",
       "      <td>88</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weather1.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reindexing using another DataFrame Index\n",
    "\n",
    "nother common technique is to reindex a DataFrame using the Index of another DataFrame. The DataFrame .reindex() method can accept the Index of a DataFrame or Series as input. You can access the Index of a DataFrame with its .index attribute.\n",
    "\n",
    "The Baby Names Dataset from data.gov summarizes counts of names (with genders) from births registered in the US since 1881. In this exercise, you will start with two baby-names DataFrames names_1981 and names_1881 loaded for you.\n",
    "\n",
    "The DataFrames names_1981 and names_1881 both have a MultiIndex with levels name and gender giving unique labels to counts in each row. If you're interested in seeing how the MultiIndexes were set up, names_1981 and names_1881 were read in using the following commands:\n",
    "\n",
    "names_1981 = pd.read_csv('names1981.csv', header=None, names=['name','gender','count'], index_col=(0,1))\n",
    "names_1881 = pd.read_csv('names1881.csv', header=None, names=['name','gender','count'], index_col=(0,1))\n",
    "As you can see by looking at their shapes, which have been printed in the IPython Shell, the DataFrame corresponding to 1981 births is much larger, reflecting the greater diversity of names in 1981 as compared to 1881.\n",
    "\n",
    "Your job here is to use the DataFrame .reindex() and .dropna() methods to make a DataFrame common_names counting names from 1881 that were still popular in 1981."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "File b'K:\\\\TensorflowPY36CPU\\\\TensorflowPY36CPU\\\\_1_PythonBasic\\\\Pandas\\\\names1981.csv' does not exist",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-91-5d250c8b1d6f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#Init\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mnames_1981\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'K:\\\\TensorflowPY36CPU\\\\TensorflowPY36CPU\\\\_1_PythonBasic\\\\Pandas\\\\names1981.csv'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheader\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnames\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'name'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'gender'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'count'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex_col\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mnames_1881\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'K:\\\\TensorflowPY36CPU\\\\TensorflowPY36CPU\\\\_1_PythonBasic\\\\Pandas\\\\names1881.csv'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheader\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnames\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'name'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'gender'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'count'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex_col\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/TensorflowCPUPY35/lib/python3.5/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36mparser_f\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, escapechar, comment, encoding, dialect, tupleize_cols, error_bad_lines, warn_bad_lines, skipfooter, skip_footer, doublequote, delim_whitespace, as_recarray, compact_ints, use_unsigned, low_memory, buffer_lines, memory_map, float_precision)\u001b[0m\n\u001b[1;32m    707\u001b[0m                     skip_blank_lines=skip_blank_lines)\n\u001b[1;32m    708\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 709\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    710\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    711\u001b[0m     \u001b[0mparser_f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/TensorflowCPUPY35/lib/python3.5/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    447\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    448\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 449\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    450\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    451\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/TensorflowCPUPY35/lib/python3.5/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m    816\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'has_index_names'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'has_index_names'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    817\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 818\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    819\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    820\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/TensorflowCPUPY35/lib/python3.5/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, engine)\u001b[0m\n\u001b[1;32m   1047\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'c'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1048\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'c'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1049\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCParserWrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1050\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1051\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'python'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/TensorflowCPUPY35/lib/python3.5/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, src, **kwds)\u001b[0m\n\u001b[1;32m   1693\u001b[0m         \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'allow_leading_cols'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex_col\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1694\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1695\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparsers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTextReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1696\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1697\u001b[0m         \u001b[0;31m# XXX\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.__cinit__\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._setup_parser_source\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: File b'K:\\\\TensorflowPY36CPU\\\\TensorflowPY36CPU\\\\_1_PythonBasic\\\\Pandas\\\\names1981.csv' does not exist"
     ],
     "output_type": "error"
    }
   ],
   "source": [
    "#Init\n",
    "\n",
    "names_1981 = pd.read_csv('K:\\\\TensorflowPY36CPU\\\\TensorflowPY36CPU\\\\_1_PythonBasic\\\\Pandas\\\\names1981.csv', header=None, names=['name','gender','count'], index_col=(0,1))\n",
    "\n",
    "names_1881 = pd.read_csv('K:\\\\TensorflowPY36CPU\\\\TensorflowPY36CPU\\\\_1_PythonBasic\\\\Pandas\\\\names1881.csv', header=None, names=['name','gender','count'], index_col=(0,1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'names_1981' is not defined",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-92-b31ca667be61>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# Reindex names_1981 with index of names_1881: common_names\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m#Create a new DataFrame common_names by reindexing names_1981 using the Index of the DataFrame names_1881 of older names.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mcommon_names\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnames_1981\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreindex\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnames_1881\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;31m# Print shape of common_names\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'names_1981' is not defined"
     ],
     "output_type": "error"
    }
   ],
   "source": [
    "# Import pandas\n",
    "import pandas as pd\n",
    "\n",
    "# Reindex names_1981 with index of names_1881: common_names\n",
    "#Create a new DataFrame common_names by reindexing names_1981 using the Index of the DataFrame names_1881 of older names.\n",
    "common_names = names_1981.reindex(names_1881.index)\n",
    "\n",
    "# Print shape of common_names\n",
    "print(common_names.shape)\n",
    "\n",
    "# Drop rows with null counts: common_names\n",
    "common_names = common_names.dropna()\n",
    "\n",
    "# Print shape of new common_names\n",
    "print(common_names.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Broadcasting in arithmetic formulas\n",
    "In this exercise, you'll work with weather data pulled from wunderground.com. The DataFrame weather has been pre-loaded along with pandas as pd. It has 365 rows (observed each day of the year 2013 in Pittsburgh, PA) and 22 columns reflecting different weather measurements each day.\n",
    "\n",
    "You'll subset a collection of columns related to temperature measurements in degrees Fahrenheit, convert them to degrees Celsius, and relabel the columns of the new DataFrame to reflect the change of units.\n",
    "\n",
    "Remember, ordinary arithmetic operators (like +, -, *, and /) broadcast scalar values to conforming DataFrames when combining scalars & DataFrames in arithmetic expressions. Broadcasting also works with pandas Series and NumPy arrays."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "File b'K:\\\\TensorflowPY36CPU\\\\TensorflowPY36CPU\\\\_1_PythonBasic\\\\Pandas\\\\/pittsburgh2013.csv' does not exist",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-93-b8495537c8cf>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mweather\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'K:\\\\TensorflowPY36CPU\\\\TensorflowPY36CPU\\\\_1_PythonBasic\\\\Pandas\\\\/pittsburgh2013.csv'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mindex_col\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'Date'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/envs/TensorflowCPUPY35/lib/python3.5/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36mparser_f\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, escapechar, comment, encoding, dialect, tupleize_cols, error_bad_lines, warn_bad_lines, skipfooter, skip_footer, doublequote, delim_whitespace, as_recarray, compact_ints, use_unsigned, low_memory, buffer_lines, memory_map, float_precision)\u001b[0m\n\u001b[1;32m    707\u001b[0m                     skip_blank_lines=skip_blank_lines)\n\u001b[1;32m    708\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 709\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    710\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    711\u001b[0m     \u001b[0mparser_f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/TensorflowCPUPY35/lib/python3.5/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    447\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    448\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 449\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    450\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    451\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/TensorflowCPUPY35/lib/python3.5/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m    816\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'has_index_names'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'has_index_names'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    817\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 818\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    819\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    820\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/TensorflowCPUPY35/lib/python3.5/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, engine)\u001b[0m\n\u001b[1;32m   1047\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'c'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1048\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'c'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1049\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCParserWrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1050\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1051\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'python'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/TensorflowCPUPY35/lib/python3.5/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, src, **kwds)\u001b[0m\n\u001b[1;32m   1693\u001b[0m         \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'allow_leading_cols'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex_col\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1694\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1695\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparsers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTextReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1696\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1697\u001b[0m         \u001b[0;31m# XXX\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.__cinit__\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._setup_parser_source\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: File b'K:\\\\TensorflowPY36CPU\\\\TensorflowPY36CPU\\\\_1_PythonBasic\\\\Pandas\\\\/pittsburgh2013.csv' does not exist"
     ],
     "output_type": "error"
    }
   ],
   "source": [
    "## init\n",
    "import pandas as pd\n",
    "\n",
    "weather = pd.read_csv('K:\\\\TensorflowPY36CPU\\\\TensorflowPY36CPU\\\\_1_PythonBasic\\\\Pandas\\\\/pittsburgh2013.csv',index_col='Date')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'weather' is not defined",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-94-2b895f4a7b99>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweather\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'weather' is not defined"
     ],
     "output_type": "error"
    }
   ],
   "source": [
    "print(weather.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'weather' is not defined",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-95-f4496b10c95e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# Extract selected columns from weather as new DataFrame: temps_f\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mtemps_f\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mweather\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Min TemperatureF'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'Mean TemperatureF'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'Max TemperatureF'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;31m# Convert temps_f to celsius: temps_c\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'weather' is not defined"
     ],
     "output_type": "error"
    }
   ],
   "source": [
    "# Excerise\n",
    "\n",
    "# Extract selected columns from weather as new DataFrame: temps_f\n",
    "temps_f = weather[['Min TemperatureF','Mean TemperatureF','Max TemperatureF']]\n",
    "\n",
    "# Convert temps_f to celsius: temps_c\n",
    "temps_c = (temps_f - 32) * 5/9\n",
    "\n",
    "# Rename 'F' in column names with 'C': temps_c.columns\n",
    "temps_c.columns =temps_c.columns.str.replace('F','C')\n",
    "\n",
    "# Print first 5 rows of temps_c\n",
    "print(temps_c.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Computing percentage growth of GDP\n",
    "Your job in this exercise is to compute the yearly percent-change of US GDP (Gross Domestic Product) since 2008.\n",
    "\n",
    "The data has been obtained from the Federal Reserve Bank of St. Louis and is available in the file GDP.csv, which contains quarterly data; you will resample it to annual sampling and then compute the annual growth of GDP. For a refresher on resampling, check out the relevant material from pandas Foundations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "File b'K:\\\\TensorflowPY36CPU\\\\TensorflowPY36CPU\\\\_1_PythonBasic\\\\Pandas\\\\/gdp_usa.csv' does not exist",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-96-49fe75b5701f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# Read 'GDP.csv' into a DataFrame: gdp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mgdp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'K:\\\\TensorflowPY36CPU\\\\TensorflowPY36CPU\\\\_1_PythonBasic\\\\Pandas\\\\/gdp_usa.csv'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mparse_dates\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex_col\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'DATE'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/envs/TensorflowCPUPY35/lib/python3.5/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36mparser_f\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, escapechar, comment, encoding, dialect, tupleize_cols, error_bad_lines, warn_bad_lines, skipfooter, skip_footer, doublequote, delim_whitespace, as_recarray, compact_ints, use_unsigned, low_memory, buffer_lines, memory_map, float_precision)\u001b[0m\n\u001b[1;32m    707\u001b[0m                     skip_blank_lines=skip_blank_lines)\n\u001b[1;32m    708\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 709\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    710\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    711\u001b[0m     \u001b[0mparser_f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/TensorflowCPUPY35/lib/python3.5/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    447\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    448\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 449\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    450\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    451\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/TensorflowCPUPY35/lib/python3.5/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m    816\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'has_index_names'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'has_index_names'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    817\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 818\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    819\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    820\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/TensorflowCPUPY35/lib/python3.5/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, engine)\u001b[0m\n\u001b[1;32m   1047\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'c'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1048\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'c'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1049\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCParserWrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1050\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1051\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'python'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/TensorflowCPUPY35/lib/python3.5/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, src, **kwds)\u001b[0m\n\u001b[1;32m   1693\u001b[0m         \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'allow_leading_cols'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex_col\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1694\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1695\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparsers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTextReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1696\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1697\u001b[0m         \u001b[0;31m# XXX\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.__cinit__\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._setup_parser_source\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: File b'K:\\\\TensorflowPY36CPU\\\\TensorflowPY36CPU\\\\_1_PythonBasic\\\\Pandas\\\\/gdp_usa.csv' does not exist"
     ],
     "output_type": "error"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Read 'GDP.csv' into a DataFrame: gdp\n",
    "gdp = pd.read_csv('K:\\\\TensorflowPY36CPU\\\\TensorflowPY36CPU\\\\_1_PythonBasic\\\\Pandas\\\\/gdp_usa.csv',parse_dates=True, index_col='DATE')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'gdp' is not defined",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-97-f5087666afc4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mgdp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'gdp' is not defined"
     ],
     "output_type": "error"
    }
   ],
   "source": [
    "gdp.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'gdp' is not defined",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-98-acb4d61eccc1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m# Slice all the gdp data from 2008 onward: post2008\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mpost2008\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgdp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'2008'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# Print the last 8 rows of post2008\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'gdp' is not defined"
     ],
     "output_type": "error"
    }
   ],
   "source": [
    "\n",
    "# Slice all the gdp data from 2008 onward: post2008\n",
    "post2008 = gdp.loc['2008':]\n",
    "\n",
    "# Print the last 8 rows of post2008\n",
    "print(post2008.tail(8))\n",
    "\n",
    "# Resample post2008 by year, keeping last(): yearly\n",
    "yearly = post2008.resample('A').last()\n",
    "\n",
    "# Print yearly\n",
    "print(yearly)\n",
    "\n",
    "# Compute percentage growth of yearly: yearly['growth']\n",
    "yearly['growth'] = yearly.pct_change()*100\n",
    "\n",
    "# Print yearly again\n",
    "print(yearly)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Converting currency of stocks\n",
    "\n",
    "n this exercise, stock prices in US Dollars for the S&P 500 in 2015 have been obtained from Yahoo Finance. The files sp500.csv for sp500 and exchange.csv for the exchange rates are both provided to you.\n",
    "\n",
    "Using the daily exchange rate to Pounds Sterling, your task is to convert both the Open and Close column prices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "File b'K:\\\\TensorflowPY36CPU\\\\TensorflowPY36CPU\\\\_1_PythonBasic\\\\Pandas\\\\sp500.csv' does not exist",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-99-301fdbd8ff8f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# Read 'sp500.csv' into a DataFrame: sp500\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0msp500\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'K:\\\\TensorflowPY36CPU\\\\TensorflowPY36CPU\\\\_1_PythonBasic\\\\Pandas\\\\sp500.csv'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mparse_dates\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mindex_col\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'Date'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;31m# Read 'exchange.csv' into a DataFrame: exchange\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/TensorflowCPUPY35/lib/python3.5/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36mparser_f\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, escapechar, comment, encoding, dialect, tupleize_cols, error_bad_lines, warn_bad_lines, skipfooter, skip_footer, doublequote, delim_whitespace, as_recarray, compact_ints, use_unsigned, low_memory, buffer_lines, memory_map, float_precision)\u001b[0m\n\u001b[1;32m    707\u001b[0m                     skip_blank_lines=skip_blank_lines)\n\u001b[1;32m    708\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 709\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    710\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    711\u001b[0m     \u001b[0mparser_f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/TensorflowCPUPY35/lib/python3.5/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    447\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    448\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 449\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    450\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    451\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/TensorflowCPUPY35/lib/python3.5/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m    816\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'has_index_names'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'has_index_names'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    817\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 818\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    819\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    820\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/TensorflowCPUPY35/lib/python3.5/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, engine)\u001b[0m\n\u001b[1;32m   1047\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'c'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1048\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'c'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1049\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCParserWrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1050\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1051\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'python'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/TensorflowCPUPY35/lib/python3.5/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, src, **kwds)\u001b[0m\n\u001b[1;32m   1693\u001b[0m         \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'allow_leading_cols'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex_col\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1694\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1695\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparsers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTextReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1696\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1697\u001b[0m         \u001b[0;31m# XXX\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.__cinit__\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._setup_parser_source\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: File b'K:\\\\TensorflowPY36CPU\\\\TensorflowPY36CPU\\\\_1_PythonBasic\\\\Pandas\\\\sp500.csv' does not exist"
     ],
     "output_type": "error"
    }
   ],
   "source": [
    "# Import pandas\n",
    "import pandas as pd\n",
    "\n",
    "# Read 'sp500.csv' into a DataFrame: sp500\n",
    "sp500 = pd.read_csv('K:\\\\TensorflowPY36CPU\\\\TensorflowPY36CPU\\\\_1_PythonBasic\\\\Pandas\\\\sp500.csv',parse_dates=True,index_col='Date')\n",
    "\n",
    "# Read 'exchange.csv' into a DataFrame: exchange\n",
    "exchange = pd.read_csv('K:\\\\TensorflowPY36CPU\\\\TensorflowPY36CPU\\\\_1_PythonBasic\\\\Pandas\\\\exchange.csv',parse_dates=True,index_col='Date')\n",
    "\n",
    "# Subset 'Open' & 'Close' columns from sp500: dollars\n",
    "dollars = sp500[['Open','Close']]\n",
    "\n",
    "# Print the head of dollars\n",
    "print(dollars.head())\n",
    "\n",
    "# Convert dollars to pounds: pounds\n",
    "pounds = dollars.multiply(exchange['GBP/USD'],axis='rows')\n",
    "\n",
    "# Print the head of pounds\n",
    "print(pounds.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    ""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "File b'K:\\\\TensorflowPY36CPU\\\\TensorflowPY36CPU\\\\_1_PythonBasic\\\\Pandas\\\\sp500.csv' does not exist",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-100-301fdbd8ff8f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# Read 'sp500.csv' into a DataFrame: sp500\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0msp500\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'K:\\\\TensorflowPY36CPU\\\\TensorflowPY36CPU\\\\_1_PythonBasic\\\\Pandas\\\\sp500.csv'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mparse_dates\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mindex_col\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'Date'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;31m# Read 'exchange.csv' into a DataFrame: exchange\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/TensorflowCPUPY35/lib/python3.5/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36mparser_f\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, escapechar, comment, encoding, dialect, tupleize_cols, error_bad_lines, warn_bad_lines, skipfooter, skip_footer, doublequote, delim_whitespace, as_recarray, compact_ints, use_unsigned, low_memory, buffer_lines, memory_map, float_precision)\u001b[0m\n\u001b[1;32m    707\u001b[0m                     skip_blank_lines=skip_blank_lines)\n\u001b[1;32m    708\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 709\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    710\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    711\u001b[0m     \u001b[0mparser_f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/TensorflowCPUPY35/lib/python3.5/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    447\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    448\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 449\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    450\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    451\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/TensorflowCPUPY35/lib/python3.5/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m    816\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'has_index_names'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'has_index_names'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    817\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 818\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    819\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    820\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/TensorflowCPUPY35/lib/python3.5/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, engine)\u001b[0m\n\u001b[1;32m   1047\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'c'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1048\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'c'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1049\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCParserWrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1050\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1051\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'python'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/TensorflowCPUPY35/lib/python3.5/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, src, **kwds)\u001b[0m\n\u001b[1;32m   1693\u001b[0m         \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'allow_leading_cols'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex_col\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1694\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1695\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparsers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTextReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1696\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1697\u001b[0m         \u001b[0;31m# XXX\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.__cinit__\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._setup_parser_source\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: File b'K:\\\\TensorflowPY36CPU\\\\TensorflowPY36CPU\\\\_1_PythonBasic\\\\Pandas\\\\sp500.csv' does not exist"
     ],
     "output_type": "error"
    }
   ],
   "source": [
    "# Import pandas\n",
    "import pandas as pd\n",
    "\n",
    "# Read 'sp500.csv' into a DataFrame: sp500\n",
    "sp500 = pd.read_csv('K:\\\\TensorflowPY36CPU\\\\TensorflowPY36CPU\\\\_1_PythonBasic\\\\Pandas\\\\sp500.csv',parse_dates=True,index_col='Date')\n",
    "\n",
    "# Read 'exchange.csv' into a DataFrame: exchange\n",
    "exchange = pd.read_csv('K:\\\\TensorflowPY36CPU\\\\TensorflowPY36CPU\\\\_1_PythonBasic\\\\Pandas\\\\exchange.csv',parse_dates=True,index_col='Date')\n",
    "\n",
    "# Subset 'Open' & 'Close' columns from sp500: dollars\n",
    "dollars = sp500[['Open','Close']]\n",
    "\n",
    "# Print the head of dollars\n",
    "print(dollars.head())\n",
    "\n",
    "# Convert dollars to pounds: pounds\n",
    "pounds = dollars.multiply(exchange['GBP/USD'],axis='rows')\n",
    "\n",
    "# Print the head of pounds\n",
    "print(pounds.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Appending pandas Series\n",
    "\n",
    "In this exercise, you'll load sales data from the months January, February, and March into DataFrames. Then, you'll extract Series with the 'Units' column from each and append them together with method chaining using .append().\n",
    "\n",
    "To check that the stacking worked, you'll print slices from these Series, and finally, you'll add the result to figure out the total units sold in the first quarter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "File b'K:\\\\TensorflowPY36CPU\\\\TensorflowPY36CPU\\\\_1_PythonBasic\\\\Pandas\\\\sales-jan-2015.csv' does not exist",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-101-cafb6593f1b0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# Load 'sales-jan-2015.csv' into a DataFrame: jan\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mjan\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'K:\\\\TensorflowPY36CPU\\\\TensorflowPY36CPU\\\\_1_PythonBasic\\\\Pandas\\\\sales-jan-2015.csv'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mparse_dates\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mindex_col\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'Date'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;31m# Load 'sales-feb-2015.csv' into a DataFrame: feb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/TensorflowCPUPY35/lib/python3.5/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36mparser_f\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, escapechar, comment, encoding, dialect, tupleize_cols, error_bad_lines, warn_bad_lines, skipfooter, skip_footer, doublequote, delim_whitespace, as_recarray, compact_ints, use_unsigned, low_memory, buffer_lines, memory_map, float_precision)\u001b[0m\n\u001b[1;32m    707\u001b[0m                     skip_blank_lines=skip_blank_lines)\n\u001b[1;32m    708\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 709\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    710\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    711\u001b[0m     \u001b[0mparser_f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/TensorflowCPUPY35/lib/python3.5/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    447\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    448\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 449\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    450\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    451\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/TensorflowCPUPY35/lib/python3.5/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m    816\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'has_index_names'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'has_index_names'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    817\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 818\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    819\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    820\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/TensorflowCPUPY35/lib/python3.5/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, engine)\u001b[0m\n\u001b[1;32m   1047\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'c'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1048\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'c'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1049\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCParserWrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1050\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1051\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'python'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/TensorflowCPUPY35/lib/python3.5/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, src, **kwds)\u001b[0m\n\u001b[1;32m   1693\u001b[0m         \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'allow_leading_cols'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex_col\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1694\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1695\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparsers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTextReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1696\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1697\u001b[0m         \u001b[0;31m# XXX\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.__cinit__\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._setup_parser_source\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: File b'K:\\\\TensorflowPY36CPU\\\\TensorflowPY36CPU\\\\_1_PythonBasic\\\\Pandas\\\\sales-jan-2015.csv' does not exist"
     ],
     "output_type": "error"
    }
   ],
   "source": [
    "# Import pandas\n",
    "import pandas as pd\n",
    "\n",
    "# Load 'sales-jan-2015.csv' into a DataFrame: jan\n",
    "jan = pd.read_csv('K:\\\\TensorflowPY36CPU\\\\TensorflowPY36CPU\\\\_1_PythonBasic\\\\Pandas\\\\sales-jan-2015.csv',parse_dates=True,index_col='Date')\n",
    "\n",
    "# Load 'sales-feb-2015.csv' into a DataFrame: feb\n",
    "feb = pd.read_csv('K:\\\\TensorflowPY36CPU\\\\TensorflowPY36CPU\\\\_1_PythonBasic\\\\Pandas\\\\sales-feb-2015.csv',parse_dates=True,index_col='Date')\n",
    "\n",
    "# Load 'sales-mar-2015.csv' into a DataFrame: mar\n",
    "mar = pd.read_csv('K:\\\\TensorflowPY36CPU\\\\TensorflowPY36CPU\\\\_1_PythonBasic\\\\Pandas\\\\sales-mar-2015.csv',parse_dates=True,index_col='Date')\n",
    "\n",
    "# Extract the 'Units' column from jan: jan_units\n",
    "jan_units = jan['Units']\n",
    "\n",
    "# Extract the 'Units' column from feb: feb_units\n",
    "feb_units = feb['Units']\n",
    "\n",
    "# Extract the 'Units' column from mar: mar_units\n",
    "mar_units = mar['Units']\n",
    "\n",
    "# Append feb_units and then mar_units to jan_units: quarter1\n",
    "quarter1 = jan_units.append(feb_units).append(mar_units)\n",
    "\n",
    "# Print the first slice from quarter1\n",
    "print(quarter1.loc['jan 27, 2015':'feb 2, 2015'])\n",
    "\n",
    "# Print the second slice from quarter1\n",
    "print(quarter1.loc['feb 26, 2015':'mar 7, 2015' ])\n",
    "\n",
    "# Compute & print total sales in quarter1\n",
    "print(quarter1.sum())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Concatenating pandas Series along row axis\n",
    "\n",
    "Having learned how to append Series, you'll now learn how to achieve the same result by concatenating Series instead. You'll continue to work with the sales data you've seen previously. This time, the DataFrames jan, feb, and mar have been pre-loaded.\n",
    "\n",
    "Your job is to use pd.concat() with a list of Series to achieve the same result that you would get by chaining calls to .append().\n",
    "\n",
    "You may be wondering about the difference between pd.concat() and pandas' .append() method. One way to think of the difference is that .append() is a specific case of a concatenation, while pd.concat() gives you more flexibility, as you'll see in later exercises.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'jan' is not defined",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-102-239e158f9f71>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# Build the list of Series\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mmonth\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mjan\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmar\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m     \u001b[0munits\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmonth\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Units'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'jan' is not defined"
     ],
     "output_type": "error"
    }
   ],
   "source": [
    "# Initialize empty list: units\n",
    "units = []\n",
    "\n",
    "# Build the list of Series\n",
    "for month in [jan, feb, mar]:\n",
    "    units.append(month['Units'])\n",
    "\n",
    "# Concatenate the list: quarter1\n",
    "quarter1 = pd.concat(units,axis='rows')\n",
    "\n",
    "# Print slices from quarter1\n",
    "print(quarter1.loc['jan 27, 2015':'feb 2, 2015'])\n",
    "print(quarter1.loc['feb 26, 2015':'mar 7, 2015'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Appending DataFrames with ignore_index\n",
    "In this exercise, you'll use the Baby Names Dataset (from data.gov) again. This time, both DataFrames names_1981 and names_1881 are loaded without specifying an Index column (so the default Indexes for both are RangeIndexes).\n",
    "\n",
    "You'll use the DataFrame .append() method to make a DataFrame combined_names. To distinguish rows from the original two DataFrames, you'll add a 'year' column to each with the year (1881 or 1981 in this case). In addition, you'll specify ignore_index=True so that the index values are not used along the concatenation axis. The resulting axis will instead be labeled 0, 1, ..., n-1, which is useful if you are concatenating objects where the concatenation axis does not have meaningful indexing information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "## init\n",
    "names_1881 = pd.read_csv(path+'names1881.csv',header=None,names=['name','gender','count'])\n",
    "names_1981 = pd.read_csv(path+'names1981.csv',header=None,names=['name','gender','count'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>gender</th>\n",
       "      <th>count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Jennifer</td>\n",
       "      <td>F</td>\n",
       "      <td>57032</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Jessica</td>\n",
       "      <td>F</td>\n",
       "      <td>42519</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Amanda</td>\n",
       "      <td>F</td>\n",
       "      <td>34370</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Sarah</td>\n",
       "      <td>F</td>\n",
       "      <td>28162</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Melissa</td>\n",
       "      <td>F</td>\n",
       "      <td>28003</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>gender</th>\n",
       "      <th>count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Jennifer</td>\n",
       "      <td>F</td>\n",
       "      <td>57032</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Jessica</td>\n",
       "      <td>F</td>\n",
       "      <td>42519</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Amanda</td>\n",
       "      <td>F</td>\n",
       "      <td>34370</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Sarah</td>\n",
       "      <td>F</td>\n",
       "      <td>28162</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Melissa</td>\n",
       "      <td>F</td>\n",
       "      <td>28003</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "names_1881.head()\n",
    "names_1981.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(19455, 4)\n(1935, 4)\n(21390, 4)\n         name gender  count  year\n1283   Morgan      M     23  1881\n2096   Morgan      F   1769  1981\n14390  Morgan      M    766  1981\n"
     ]
    }
   ],
   "source": [
    "# Add 'year' column to names_1881 and names_1981\n",
    "names_1881['year'] = 1881\n",
    "names_1981['year'] = 1981\n",
    "\n",
    "# Append names_1981 after names_1881 with ignore_index=True: combined_names\n",
    "combined_names = names_1881.append(names_1981,ignore_index=True)\n",
    "\n",
    "# Print shapes of names_1981, names_1881, and combined_names\n",
    "print(names_1981.shape)\n",
    "print(names_1881.shape)\n",
    "print(combined_names.shape)\n",
    "\n",
    "# Print all rows that contain the name 'Morgan'\n",
    "print(combined_names.loc[combined_names['name']=='Morgan'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Concatenating pandas DataFrames along column axis\n",
    "The function pd.concat() can concatenate DataFrames horizontally as well as vertically (vertical is the default). To make the DataFrames stack horizontally, you have to specify the keyword argument axis=1 or axis='columns'.\n",
    "\n",
    "In this exercise, you'll use weather data with maximum and mean daily temperatures sampled at different rates (quarterly versus monthly). You'll concatenate the rows of both and see that, where rows are missing in the coarser DataFrame, null values are inserted in the concatenated DataFrame. This corresponds to an outer join (which you will explore in more detail in later exercises).\n",
    "\n",
    "The files 'quarterly_max_temp.csv' and 'monthly_mean_temp.csv' have been pre-loaded into the DataFrames weather_max and weather_mean respectively, and pandas has been imported as pd."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "File b'/Users/victor/Documents/Dev/TensorflowPY36CPU/_1_PythonBasic/Pandas/monthly_mean_temp.csv' does not exist",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-106-6f6e997e13b3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#init\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mweather_max\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m'quarterly_max_temp.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mweather_mean\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m'monthly_mean_temp.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mweather_max\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mweather_mean\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/TensorflowCPUPY35/lib/python3.5/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36mparser_f\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, escapechar, comment, encoding, dialect, tupleize_cols, error_bad_lines, warn_bad_lines, skipfooter, skip_footer, doublequote, delim_whitespace, as_recarray, compact_ints, use_unsigned, low_memory, buffer_lines, memory_map, float_precision)\u001b[0m\n\u001b[1;32m    707\u001b[0m                     skip_blank_lines=skip_blank_lines)\n\u001b[1;32m    708\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 709\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    710\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    711\u001b[0m     \u001b[0mparser_f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/TensorflowCPUPY35/lib/python3.5/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    447\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    448\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 449\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    450\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    451\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/TensorflowCPUPY35/lib/python3.5/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m    816\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'has_index_names'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'has_index_names'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    817\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 818\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    819\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    820\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/TensorflowCPUPY35/lib/python3.5/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, engine)\u001b[0m\n\u001b[1;32m   1047\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'c'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1048\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'c'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1049\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCParserWrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1050\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1051\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'python'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/TensorflowCPUPY35/lib/python3.5/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, src, **kwds)\u001b[0m\n\u001b[1;32m   1693\u001b[0m         \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'allow_leading_cols'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex_col\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1694\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1695\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparsers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTextReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1696\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1697\u001b[0m         \u001b[0;31m# XXX\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.__cinit__\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._setup_parser_source\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: File b'/Users/victor/Documents/Dev/TensorflowPY36CPU/_1_PythonBasic/Pandas/monthly_mean_temp.csv' does not exist"
     ],
     "output_type": "error"
    }
   ],
   "source": [
    "#init\n",
    "weather_max = pd.read_csv(path+'quarterly_max_temp.csv')\n",
    "weather_mean = pd.read_csv(path+'monthly_mean_temp.csv')\n",
    "weather_max.head()\n",
    "weather_mean.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'weather_mean' is not defined",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-107-4f688a5f775d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Concatenate weather_max and weather_mean horizontally: weather\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mweather\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mweather_max\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweather_mean\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m,\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# Print weather\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweather\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'weather_mean' is not defined"
     ],
     "output_type": "error"
    }
   ],
   "source": [
    "# Concatenate weather_max and weather_mean horizontally: weather\n",
    "weather = pd.concat([weather_max, weather_mean] ,axis=1)\n",
    "\n",
    "# Print weather\n",
    "print(weather)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading multiple files to build a DataFrame\n",
    "\n",
    "Reading multiple files to build a DataFrame\n",
    "It is often convenient to build a large DataFrame by parsing many files as DataFrames and concatenating them all at once. You'll do this here with three files, but, in principle, this approach can be used to combine data from dozens or hundreds of files.\n",
    "\n",
    "Here, you'll work with DataFrames compiled from The Guardian's Olympic medal dataset.\n",
    "\n",
    "pandas has been imported as pd and two lists have been pre-loaded: An empty list called medals, and medal_types, which contains the strings 'bronze', 'silver', and 'gold'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "# init\n",
    "\n",
    "medals = []\n",
    "medal_types = ['bronze', 'silver', 'gold']\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                bronze  silver    gold\nFrance           475.0   461.0     NaN\nGermany          454.0     NaN   407.0\nItaly              NaN   394.0   460.0\nSoviet Union     584.0   627.0   838.0\nUnited Kingdom   505.0   591.0   498.0\nUnited States   1052.0  1195.0  2088.0\n"
     ]
    }
   ],
   "source": [
    "for medal in medal_types:\n",
    "\n",
    "    # Create the file name: file_name\n",
    "    file_name = path+\"%s_top5.csv\" % medal\n",
    "    \n",
    "    # Create list of column names: columns\n",
    "    columns = ['Country', medal]\n",
    "    \n",
    "    # Read file_name into a DataFrame: df\n",
    "    medal_df = pd.read_csv(file_name,header=0,index_col='Country',names=columns)\n",
    "\n",
    "    # Append medal_df to medals\n",
    "    medals.append(medal_df)\n",
    "\n",
    "# Concatenate medals horizontally: medals\n",
    "medals = pd.concat(medals,axis='columns')\n",
    "\n",
    "# Print medals\n",
    "print(medals)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Concatenating vertically to get MultiIndexed rows\n",
    "When stacking a sequence of DataFrames vertically, it is sometimes desirable to construct a MultiIndex to indicate the DataFrame from which each row originated. This can be done by specifying the keys parameter in the call to pd.concat(), which generates a hierarchical index with the labels from keys as the outermost index label. So you don't have to rename the columns of each DataFrame as you load it. Instead, only the Index column needs to be specified.\n",
    "\n",
    "Here, you'll continue working with DataFrames compiled from The Guardian's Olympic medal dataset. Once again, pandas has been imported as pd and two lists have been pre-loaded: An empty list called medals, and medal_types, which contains the strings 'bronze', 'silver', and 'gold'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "# init\n",
    "\n",
    "medals = []\n",
    "medal_types = ['bronze', 'silver', 'gold']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                        Total\n       Country               \nbronze United States   1052.0\n       Soviet Union     584.0\n       United Kingdom   505.0\n       France           475.0\n       Germany          454.0\nsilver United States   1195.0\n       Soviet Union     627.0\n       United Kingdom   591.0\n       France           461.0\n       Italy            394.0\ngold   United States   2088.0\n       Soviet Union     838.0\n       United Kingdom   498.0\n       Italy            460.0\n       Germany          407.0\n"
     ]
    }
   ],
   "source": [
    "for medal in medal_types:\n",
    "\n",
    "    file_name = path+\"%s_top5.csv\" % medal\n",
    "    \n",
    "    # Read file_name into a DataFrame: medal_df\n",
    "    medal_df = pd.read_csv(file_name,index_col='Country')\n",
    "    \n",
    "    # Append medal_df to medals\n",
    "    medals.append(medal_df)\n",
    "    \n",
    "    \n",
    "# Concatenate medals: medals\n",
    "medals = pd.concat(medals,keys=['bronze', 'silver', 'gold'] )\n",
    "\n",
    "# Print medals in entirety\n",
    "print(medals)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## This exercise picks up where the last ended (again using The Guardian's Olympic medal dataset).\n",
    "\n",
    "You are provided with the MultiIndexed DataFrame as produced at the end of the preceding exercise. Your task is to sort the DataFrame and to use the pd.IndexSlice to extract specific slices. Check out this exercise from Manipulating DataFrames with pandas to refresh your memory on how to deal with MultiIndexed DataFrames.\n",
    "\n",
    "pandas has been imported for you as pd and the DataFrame medals is already in your namespace.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sort the entries of medals: medals_sorted\n",
    "medals_sorted = medals.sort_index(level=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>Total</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th>Country</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"5\" valign=\"top\">bronze</th>\n",
       "      <th>France</th>\n",
       "      <td>475.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Germany</th>\n",
       "      <td>454.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Soviet Union</th>\n",
       "      <td>584.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>United Kingdom</th>\n",
       "      <td>505.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>United States</th>\n",
       "      <td>1052.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"5\" valign=\"top\">gold</th>\n",
       "      <th>Germany</th>\n",
       "      <td>407.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Italy</th>\n",
       "      <td>460.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Soviet Union</th>\n",
       "      <td>838.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>United Kingdom</th>\n",
       "      <td>498.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>United States</th>\n",
       "      <td>2088.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"5\" valign=\"top\">silver</th>\n",
       "      <th>France</th>\n",
       "      <td>461.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Italy</th>\n",
       "      <td>394.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Soviet Union</th>\n",
       "      <td>627.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>United Kingdom</th>\n",
       "      <td>591.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>United States</th>\n",
       "      <td>1195.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>Total</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th>Country</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"5\" valign=\"top\">bronze</th>\n",
       "      <th>France</th>\n",
       "      <td>475.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Germany</th>\n",
       "      <td>454.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Soviet Union</th>\n",
       "      <td>584.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>United Kingdom</th>\n",
       "      <td>505.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>United States</th>\n",
       "      <td>1052.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"5\" valign=\"top\">gold</th>\n",
       "      <th>Germany</th>\n",
       "      <td>407.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Italy</th>\n",
       "      <td>460.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Soviet Union</th>\n",
       "      <td>838.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>United Kingdom</th>\n",
       "      <td>498.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>United States</th>\n",
       "      <td>2088.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"5\" valign=\"top\">silver</th>\n",
       "      <th>France</th>\n",
       "      <td>461.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Italy</th>\n",
       "      <td>394.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Soviet Union</th>\n",
       "      <td>627.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>United Kingdom</th>\n",
       "      <td>591.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>United States</th>\n",
       "      <td>1195.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "medals_sorted\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total    454.0\nName: (bronze, Germany), dtype: float64\n                 Total\nCountry               \nFrance           461.0\nItaly            394.0\nSoviet Union     627.0\nUnited Kingdom   591.0\nUnited States   1195.0\n                       Total\n       Country              \nbronze United Kingdom  505.0\ngold   United Kingdom  498.0\nsilver United Kingdom  591.0\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Print the number of Bronze medals won by Germany\n",
    "print(medals_sorted.loc[('bronze','Germany')])\n",
    "\n",
    "# Print data about silver medals\n",
    "print(medals_sorted.loc['silver'])\n",
    "\n",
    "# Create alias for pd.IndexSlice: idx\n",
    "idx = pd.IndexSlice\n",
    "\n",
    "# Print all the data on medals won by the United Kingdom\n",
    "print(medals_sorted.loc[idx[:, 'United Kingdom'], :])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Concatenating horizontally to get MultiIndexed columns\n",
    "It is also possible to construct a DataFrame with hierarchically indexed columns. For this exercise, you'll start with pandas imported and a list of three DataFrames called dataframes. All three DataFrames contain 'Company', 'Product', and 'Units' columns with a 'Date' column as the index pertaining to sales transactions during the month of February, 2015. The first DataFrame describes Hardware transactions, the second describes Software transactions, and the third, Service transactions.\n",
    "\n",
    "Your task is to concatenate the DataFrames horizontally and to create a MultiIndex on the columns. From there, you can summarize the resulting DataFrame and slice some information from it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[                             Company   Product  Units\n Date                                                 \n 2015-02-04 21:52:45  Acme Coporation  Hardware     14\n 2015-02-07 22:58:10  Acme Coporation  Hardware      1\n 2015-02-19 10:59:33        Mediacore  Hardware     16\n 2015-02-02 20:54:49        Mediacore  Hardware      9\n 2015-02-21 20:41:47            Hooli  Hardware      3,\n                              Company   Product  Units\n Date                                                 \n 2015-02-16 12:09:19            Hooli  Software     10\n 2015-02-03 14:14:18          Initech  Software     13\n 2015-02-02 08:33:01            Hooli  Software      3\n 2015-02-05 01:53:06  Acme Coporation  Software     19\n 2015-02-11 20:03:08          Initech  Software      7\n 2015-02-09 13:09:55        Mediacore  Software      7\n 2015-02-11 22:50:44            Hooli  Software      4\n 2015-02-04 15:36:29        Streeplex  Software     13\n 2015-02-21 05:01:26        Mediacore  Software      3,\n                        Company  Product  Units\n Date                                          \n 2015-02-26 08:57:45  Streeplex  Service      4\n 2015-02-25 00:29:00    Initech  Service     10\n 2015-02-09 08:57:30  Streeplex  Service     19\n 2015-02-26 08:58:51  Streeplex  Service      1\n 2015-02-05 22:05:03      Hooli  Service     10\n 2015-02-19 16:02:58  Mediacore  Service     10]"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#init\n",
    "Hardware = pd.read_csv(path+'feb-sales-Hardware.csv',index_col='Date')\n",
    "Software = pd.read_csv(path+'feb-sales-Software.csv',index_col='Date')\n",
    "Service= pd.read_csv(path+'feb-sales-Service.csv',index_col='Date')\n",
    "dataframes = [Hardware,Software,Service]\n",
    "\n",
    "dataframes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concatenate dataframes: february\n",
    "february = pd.concat(dataframes, axis=1, keys=['Hardware', 'Software', 'Service'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\nIndex: 20 entries, 2015-02-02 08:33:01 to 2015-02-26 08:58:51\nData columns (total 9 columns):\n(Hardware, Company)    5 non-null object\n(Hardware, Product)    5 non-null object\n(Hardware, Units)      5 non-null float64\n(Software, Company)    9 non-null object\n(Software, Product)    9 non-null object\n(Software, Units)      9 non-null float64\n(Service, Company)     6 non-null object\n(Service, Product)     6 non-null object\n(Service, Units)       6 non-null float64\ndtypes: float64(3), object(6)\nmemory usage: 1.6+ KB\n                            Hardware                   Software            \\\n                             Company   Product Units    Company   Product   \n2015-02-02 08:33:01              NaN       NaN   NaN      Hooli  Software   \n2015-02-02 20:54:49        Mediacore  Hardware   9.0        NaN       NaN   \n2015-02-03 14:14:18              NaN       NaN   NaN    Initech  Software   \n2015-02-04 15:36:29              NaN       NaN   NaN  Streeplex  Software   \n2015-02-04 21:52:45  Acme Coporation  Hardware  14.0        NaN       NaN   \n\n                          Service                \n                    Units Company Product Units  \n2015-02-02 08:33:01   3.0     NaN     NaN   NaN  \n2015-02-02 20:54:49   NaN     NaN     NaN   NaN  \n2015-02-03 14:14:18  13.0     NaN     NaN   NaN  \n2015-02-04 15:36:29  13.0     NaN     NaN   NaN  \n2015-02-04 21:52:45   NaN     NaN     NaN   NaN  \n"
     ]
    }
   ],
   "source": [
    "february.info()\n",
    "print(february.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\nIndex: 20 entries, 2015-02-02 08:33:01 to 2015-02-26 08:58:51\nData columns (total 9 columns):\n(Hardware, Company)    5 non-null object\n(Hardware, Product)    5 non-null object\n(Hardware, Units)      5 non-null float64\n(Software, Company)    9 non-null object\n(Software, Product)    9 non-null object\n(Software, Units)      9 non-null float64\n(Service, Company)     6 non-null object\n(Service, Product)     6 non-null object\n(Service, Units)       6 non-null float64\ndtypes: float64(3), object(6)\nmemory usage: 1.6+ KB\nNone\nEmpty DataFrame\nColumns: [(Hardware, Company), (Software, Company), (Service, Company)]\nIndex: []\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Print february.info()\n",
    "print(february.info())\n",
    "\n",
    "# Assign pd.IndexSlice: idx\n",
    "idx = pd.IndexSlice\n",
    "\n",
    "# Create the slice: slice_2_8\n",
    "#Extract a slice called slice_2_8 from february (using .loc[] & idx) that comprises rows between Feb. 2, 2015 to Feb. 8, 2015 from columns under 'Company'.\n",
    "slice_2_8 = february.loc['Feb 2,2015':'Feb 8,2015', idx[:, 'Company']]\n",
    "\n",
    "# Print slice_2_8\n",
    "print(slice_2_8)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Empty DataFrame\nColumns: [(Hardware, Company), (Software, Company), (Service, Company)]\nIndex: []\n"
     ]
    }
   ],
   "source": [
    "# Print slice_2_8\n",
    "print(slice_2_8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Concatenating DataFrames from a dict\n",
    "\n",
    "You're now going to revisit the sales data you worked with earlier in the chapter. Three DataFrames jan, feb, and mar have been pre-loaded for you. Your task is to aggregate the sum of all sales over the 'Company' column into a single DataFrame. You'll do this by constructing a dictionary of these DataFrames and then concatenating them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Concatenating DataFrames with inner join\n",
    "Concatenating DataFrames with inner join\n",
    "Here, you'll continue working with DataFrames compiled from The Guardian's Olympic medal dataset.\n",
    "\n",
    "The DataFrames bronze, silver, and gold have been pre-loaded for you.\n",
    "\n",
    "Your task is to compute an inner join."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'jan' is not defined",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-120-e7c2cf3e3d11>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Make the list of tuples: month_list\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m#Create a list called month_list consisting of the tuples ('january', jan), ('february', feb), and ('march', mar).\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mmonth_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'january'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mjan\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'february'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'march'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmar\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# Create an empty dictionary: month_dict\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'jan' is not defined"
     ],
     "output_type": "error"
    }
   ],
   "source": [
    "# Make the list of tuples: month_list\n",
    "#Create a list called month_list consisting of the tuples ('january', jan), ('february', feb), and ('march', mar).\n",
    "month_list = [('january', jan),('february', feb),('march', mar)]\n",
    "\n",
    "# Create an empty dictionary: month_dict\n",
    "month_dict = dict()\n",
    "\n",
    "for month_name, month_data in month_list:\n",
    "\n",
    "    # Group month_data: month_dict[month_name]\n",
    "    month_dict[month_name] = month_data.groupby('Company').sum()\n",
    "\n",
    "# Concatenate data in month_dict: sales\n",
    "sales = pd.concat(month_dict)\n",
    "\n",
    "# Print sales\n",
    "print(sales)\n",
    "\n",
    "# Print all sales by Mediacore\n",
    "idx = pd.IndexSlice\n",
    "print(sales.loc[idx[:, 'Mediacore'], :])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Concatenating DataFrames with inner join\n",
    "Concatenating DataFrames with inner join\n",
    "Here, you'll continue working with DataFrames compiled from The Guardian's Olympic medal dataset.\n",
    "\n",
    "The DataFrames bronze, silver, and gold have been pre-loaded for you.\n",
    "\n",
    "Your task is to compute an inner join."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    bronze                                silver                         \\\n       NOC                Country   Total    NOC                Country   \n0      USA          United States  1052.0    USA          United States   \n1      URS           Soviet Union   584.0    URS           Soviet Union   \n2      GBR         United Kingdom   505.0    GBR         United Kingdom   \n3      FRA                 France   475.0    FRA                 France   \n4      GER                Germany   454.0    GER                Germany   \n5      AUS              Australia   413.0    AUS              Australia   \n6      ITA                  Italy   374.0    ITA                  Italy   \n7      HUN                Hungary   345.0    HUN                Hungary   \n8      SWE                 Sweden   325.0    SWE                 Sweden   \n9      NED            Netherlands   320.0    NED            Netherlands   \n10     ROU                Romania   282.0    ROU                Romania   \n11     JPN                  Japan   270.0    JPN                  Japan   \n12     RUS                 Russia   240.0    RUS                 Russia   \n13     CAN                 Canada   227.0    CAN                 Canada   \n14     GDR           East Germany   225.0    GDR           East Germany   \n15     POL                 Poland   223.0    POL                 Poland   \n16     FIN                Finland   209.0    FIN                Finland   \n17     CHN                  China   193.0    CHN                  China   \n18     FRG           West Germany   180.0    FRG           West Germany   \n19     BRA                 Brazil   174.0    BRA                 Brazil   \n20     DEN                Denmark   152.0    DEN                Denmark   \n21     BEL                Belgium   150.0    BEL                Belgium   \n22     NOR                 Norway   144.0    NOR                 Norway   \n23     SUI            Switzerland   138.0    SUI            Switzerland   \n24     BUL               Bulgaria   136.0    BUL               Bulgaria   \n25     KOR           Korea, South   135.0    KOR           Korea, South   \n26     YUG             Yugoslavia   118.0    YUG             Yugoslavia   \n27     CUB                   Cuba   109.0    CUB                   Cuba   \n28     TCH         Czechoslovakia   105.0    TCH         Czechoslovakia   \n29     ESP                  Spain    92.0    ESP                  Spain   \n..     ...                    ...     ...    ...                    ...   \n108    BAR               Barbados     1.0    BAR               Barbados   \n109    BER               Bermuda*     1.0    BER               Bermuda*   \n110    DJI               Djibouti     1.0    DJI               Djibouti   \n111    ERI                Eritrea     1.0    ERI                Eritrea   \n112    GUY                 Guyana     1.0    GUY                 Guyana   \n113    IRQ                   Iraq     1.0    IRQ                   Iraq   \n114    KUW                 Kuwait     1.0    KUW                 Kuwait   \n115    MKD              Macedonia     1.0    MKD              Macedonia   \n116    MRI              Mauritius     1.0    MRI              Mauritius   \n117    NIG                  Niger     1.0    NIG                  Niger   \n118    TOG                   Togo     1.0    TOG                   Togo   \n119    PAR               Paraguay     NaN    PAR               Paraguay   \n120    PER                   Peru     NaN    PER                   Peru   \n121    SCG                 Serbia     NaN    SCG                 Serbia   \n122    NAM                Namibia     NaN    NAM                Namibia   \n123    SIN              Singapore     NaN    SIN              Singapore   \n124    HKG             Hong Kong*     NaN    HKG             Hong Kong*   \n125    SRI              Sri Lanka     NaN    SRI              Sri Lanka   \n126    TAN               Tanzania     NaN    TAN               Tanzania   \n127    VIE                Vietnam     NaN    VIE                Vietnam   \n128    ECU                Ecuador     NaN    ECU                Ecuador   \n129    LUX             Luxembourg     NaN    LUX             Luxembourg   \n130    AHO  Netherlands Antilles*     NaN    AHO  Netherlands Antilles*   \n131    CIV          Cote d'Ivoire     NaN    CIV          Cote d'Ivoire   \n132    ISV        Virgin Islands*     NaN    ISV        Virgin Islands*   \n133    SEN                Senegal     NaN    SEN                Senegal   \n134    SUD                  Sudan     NaN    SUD                  Sudan   \n135    TGA                  Tonga     NaN    TGA                  Tonga   \n136    BDI                Burundi     NaN    BDI                Burundi   \n137    UAE   United Arab Emirates     NaN    UAE   United Arab Emirates   \n\n            gold                                 \n      Total  NOC                Country   Total  \n0    1195.0  USA          United States  2088.0  \n1     627.0  URS           Soviet Union   838.0  \n2     591.0  GBR         United Kingdom   498.0  \n3     461.0  FRA                 France   378.0  \n4     350.0  GER                Germany   407.0  \n5     369.0  AUS              Australia   293.0  \n6     394.0  ITA                  Italy   460.0  \n7     308.0  HUN                Hungary   400.0  \n8     349.0  SWE                 Sweden   347.0  \n9     250.0  NED            Netherlands   212.0  \n10    187.0  ROU                Romania   155.0  \n11    228.0  JPN                  Japan   206.0  \n12    206.0  RUS                 Russia   192.0  \n13    211.0  CAN                 Canada   154.0  \n14    271.0  GDR           East Germany   329.0  \n15    173.0  POL                 Poland   103.0  \n16    118.0  FIN                Finland   124.0  \n17    252.0  CHN                  China   234.0  \n18    167.0  FRG           West Germany   143.0  \n19    139.0  BRA                 Brazil    59.0  \n20    192.0  DEN                Denmark   147.0  \n21    167.0  BEL                Belgium    91.0  \n22    199.0  NOR                 Norway   194.0  \n23    165.0  SUI            Switzerland    73.0  \n24    142.0  BUL               Bulgaria    53.0  \n25    191.0  KOR           Korea, South   140.0  \n26    174.0  YUG             Yugoslavia   143.0  \n27    126.0  CUB                   Cuba   160.0  \n28    144.0  TCH         Czechoslovakia    80.0  \n29    193.0  ESP                  Spain    92.0  \n..      ...  ...                    ...     ...  \n108     NaN  BAR               Barbados     NaN  \n109     NaN  BER               Bermuda*     NaN  \n110     NaN  DJI               Djibouti     NaN  \n111     NaN  ERI                Eritrea     NaN  \n112     NaN  GUY                 Guyana     NaN  \n113     NaN  IRQ                   Iraq     NaN  \n114     NaN  KUW                 Kuwait     NaN  \n115     NaN  MKD              Macedonia     NaN  \n116     NaN  MRI              Mauritius     NaN  \n117     NaN  NIG                  Niger     NaN  \n118     NaN  TOG                   Togo     NaN  \n119    17.0  PAR               Paraguay     NaN  \n120    14.0  PER                   Peru     1.0  \n121    14.0  SCG                 Serbia     NaN  \n122     4.0  NAM                Namibia     NaN  \n123     4.0  SIN              Singapore     NaN  \n124     2.0  HKG             Hong Kong*     1.0  \n125     2.0  SRI              Sri Lanka     NaN  \n126     2.0  TAN               Tanzania     NaN  \n127     2.0  VIE                Vietnam     NaN  \n128     1.0  ECU                Ecuador     1.0  \n129     1.0  LUX             Luxembourg     1.0  \n130     1.0  AHO  Netherlands Antilles*     NaN  \n131     1.0  CIV          Cote d'Ivoire     NaN  \n132     1.0  ISV        Virgin Islands*     NaN  \n133     1.0  SEN                Senegal     NaN  \n134     1.0  SUD                  Sudan     NaN  \n135     1.0  TGA                  Tonga     NaN  \n136     NaN  BDI                Burundi     1.0  \n137     NaN  UAE   United Arab Emirates     1.0  \n\n[138 rows x 9 columns]\n"
     ]
    }
   ],
   "source": [
    "# Create the list of DataFrames: medal_list\n",
    "#Construct a list of DataFrames called medal_list with entries bronze, silver, and gold.\n",
    "medal_list = [bronze,silver,gold]\n",
    "\n",
    "# Concatenate medal_list horizontally using an inner join: medals\n",
    "medals = pd.concat(medal_list,keys=['bronze', 'silver', 'gold'],axis=1,join='inner')\n",
    "\n",
    "# Print medals\n",
    "print(medals)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Resampling & concatenating DataFrames with inner join\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>US</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Year</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1947-04-01</th>\n",
       "      <td>246.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1947-07-01</th>\n",
       "      <td>250.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1947-10-01</th>\n",
       "      <td>260.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1948-01-01</th>\n",
       "      <td>266.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1948-04-01</th>\n",
       "      <td>272.9</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>US</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Year</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1947-04-01</th>\n",
       "      <td>246.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1947-07-01</th>\n",
       "      <td>250.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1947-10-01</th>\n",
       "      <td>260.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1948-01-01</th>\n",
       "      <td>266.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1948-04-01</th>\n",
       "      <td>272.9</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#init\n",
    "china = pd.read_csv(path+'china.csv',index_col='Year',parse_dates=True)\n",
    "us = pd.read_csv(path+'usa.csv',index_col='Year',parse_dates=True)\n",
    "\n",
    "china.head()\n",
    "us.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/victor/anaconda3/envs/TensorflowCPUPY35/lib/python3.5/site-packages/ipykernel_launcher.py:2: FutureWarning: \n.resample() is now a deferred operation\nYou called pct_change(...) on this deferred object which materialized it into a dataframe\nby implicitly taking the mean.  Use .resample(...).mean() instead\n  \n"
     ]
    }
   ],
   "source": [
    "# Resample and tidy china: china_annual\n",
    "china_annual = china.resample('A').pct_change(10).dropna()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Empty DataFrame\nColumns: [China, US]\nIndex: []\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/victor/anaconda3/envs/TensorflowCPUPY35/lib/python3.5/site-packages/ipykernel_launcher.py:4: FutureWarning: \n.resample() is now a deferred operation\nYou called pct_change(...) on this deferred object which materialized it into a dataframe\nby implicitly taking the mean.  Use .resample(...).mean() instead\n  after removing the cwd from sys.path.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Resample and tidy us: us_annual\n",
    "us_annual = us.resample('A').pct_change(10).dropna()\n",
    "\n",
    "# Concatenate china_annual and us_annual: gdp\n",
    "gdp = pd.concat([china_annual,us_annual],axis=1,join='inner')\n",
    "\n",
    "# Resample gdp and print\n",
    "print(gdp.resample('10A').last())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Merging on a specific column\n",
    "\n",
    "\n",
    "This exercise follows on the last one with the DataFrames revenue and managers for your company. You expect your company to grow and, eventually, to operate in cities with the same name on different states. As such, you decide that every branch should have a numerical branch identifier. Thus, you add a branch_id column to both DataFrames. Moreover, new cities have been added to both the revenue and managers DataFrames as well. pandas has been imported as pd and both DataFrames are available in your namespace.\n",
    "\n",
    "At present, there should be a 1-to-1 relationship between the city and branch_id fields. In that case, the result of a merge on the city columns ought to give you the same output as a merge on the branch_id columns. Do they? Can you spot an ambiguity in one of the DataFrames?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "#init \n",
    "\n",
    "managers = pd.read_csv(path+'managers.csv')\n",
    "revenue = pd.read_csv(path+'revenue.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>branch_id</th>\n",
       "      <th>city</th>\n",
       "      <th>manager</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>10</td>\n",
       "      <td>Austin</td>\n",
       "      <td>Charlers</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>20</td>\n",
       "      <td>Denver</td>\n",
       "      <td>Joel</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>47</td>\n",
       "      <td>Mendocino</td>\n",
       "      <td>Brett</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>31</td>\n",
       "      <td>Springfield</td>\n",
       "      <td>Sally</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>branch_id</th>\n",
       "      <th>city</th>\n",
       "      <th>manager</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>10</td>\n",
       "      <td>Austin</td>\n",
       "      <td>Charlers</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>20</td>\n",
       "      <td>Denver</td>\n",
       "      <td>Joel</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>47</td>\n",
       "      <td>Mendocino</td>\n",
       "      <td>Brett</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>31</td>\n",
       "      <td>Springfield</td>\n",
       "      <td>Sally</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "managers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>branch_id</th>\n",
       "      <th>city</th>\n",
       "      <th>revenue</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>10</td>\n",
       "      <td>Austin</td>\n",
       "      <td>100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>20</td>\n",
       "      <td>Denver</td>\n",
       "      <td>83</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>30</td>\n",
       "      <td>Springfield</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>47</td>\n",
       "      <td>Mendocino</td>\n",
       "      <td>200</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>branch_id</th>\n",
       "      <th>city</th>\n",
       "      <th>revenue</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>10</td>\n",
       "      <td>Austin</td>\n",
       "      <td>100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>20</td>\n",
       "      <td>Denver</td>\n",
       "      <td>83</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>30</td>\n",
       "      <td>Springfield</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>47</td>\n",
       "      <td>Mendocino</td>\n",
       "      <td>200</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "revenue\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   branch_id_x         city  revenue  branch_id_y   manager\n0           10  Austin           100           10  Charlers\n1           20  Denver            83           20      Joel\n2           30  Springfield        4           31     Sally\n3           47  Mendocino        200           47     Brett\n   branch_id       city_x  revenue       city_y   manager\n0         10  Austin           100  Austin       Charlers\n1         20  Denver            83  Denver           Joel\n2         47  Mendocino        200  Mendocino       Brett\n"
     ]
    }
   ],
   "source": [
    "# Merge revenue with managers on 'city': merge_by_city\n",
    "merge_by_city = pd.merge(revenue,managers,on='city')\n",
    "\n",
    "# Print merge_by_city\n",
    "print(merge_by_city)\n",
    "\n",
    "# Merge revenue with managers on 'branch_id': merge_by_id\n",
    "merge_by_id = pd.merge(revenue,managers,on='branch_id')\n",
    "\n",
    "# Print merge_by_id\n",
    "print(merge_by_id)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Merging on columns with non-matching labels\n",
    "You continue working with the revenue & managers DataFrames from before. This time, someone has changed the field name 'city' to 'branch' in the managers table. Now, when you attempt to merge DataFrames, an exception is thrown:\n",
    "\n",
    ">>> pd.merge(revenue, managers, on='city')\n",
    "Traceback (most recent call last):\n",
    "    ... <text deleted> ...\n",
    "    pd.merge(revenue, managers, on='city')\n",
    "    ... <text deleted> ...\n",
    "KeyError: 'city'\n",
    "Given this, it will take a bit more work for you to join or merge on the city/branch name. You have to specify the left_on and right_on parameters in the call to pd.merge().\n",
    "\n",
    "As before, pandas has been pre-imported as pd and the revenue and managers DataFrames are in your namespace. They have been printed in the IPython Shell so you can examine the columns prior to merging.\n",
    "\n",
    "Are you able to merge better than in the last exercise? How should the rows with Springfield be handled?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "#init\n",
    "\n",
    "managers = pd.read_csv(path+'managers2.csv')\n",
    "revenue = pd.read_csv(path+'revenue2.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   branch_id         city  revenue     state_x       branch  branch_id    \\\n0         10  Austin               100      TX  Austin                10   \n1         20  Denver                83      CO  Denver                20   \n2         30  Springfield            4      IL  Springfield           31   \n3         47  Mendocino            200      CA  Mendocino             47   \n\n    manager state_y  \n0  Charlers      TX  \n1  Joel          CO  \n2  Sally         MO  \n3  Brett         CA  \n"
     ]
    }
   ],
   "source": [
    "# Merge revenue & managers on 'city' & 'branch': combined\n",
    "combined = pd.merge(revenue,managers,left_on='city',right_on='branch')\n",
    "\n",
    "# Print combined\n",
    "print(combined)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Merging on multiple columns\n",
    "\n",
    "Another strategy to disambiguate cities with identical names is to add information on the states in which the cities are located. To this end, you add a column called state to both DataFrames from the preceding exercises. Again, pandas has been pre-imported as pd and the revenue and managers DataFrames are in your namespace.\n",
    "\n",
    "Your goal in this exercise is to use pd.merge() to merge DataFrames using multiple columns (using 'branch_id', 'city', and 'state' in this case).\n",
    "\n",
    "Are you able to match all your company's branches correctly?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "managers = pd.read_csv(path+'managers.csv')\n",
    "revenue = pd.read_csv(path+'revenue.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   branch_id         city  revenue state   manager\n0         10  Austin           100    TX  Charlers\n1         20  Denver            83    CO      Joel\n2         47  Mendocino        200    CA     Brett\n"
     ]
    }
   ],
   "source": [
    "# Add 'state' column to revenue: revenue['state']\n",
    "revenue['state'] = ['TX','CO','IL','CA']\n",
    "\n",
    "# Add 'state' column to managers: managers['state']\n",
    "managers['state'] = ['TX','CO','CA','MO']\n",
    "\n",
    "# Merge revenue & managers on 'branch_id', 'city', & 'state': combined\n",
    "combined = pd.merge(revenue,managers,on=['branch_id', 'city', 'state'])\n",
    "\n",
    "# Print combined\n",
    "print(combined)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Which join should I use?\n",
    " - df1.append(df2) : stack vertically\n",
    " - pd.concat([df1,df2])\n",
    "    stack vertical or horizontally\n",
    "    simple inner/out join on indexes\n",
    " - df1.join(df2): inner/outer/left/right join on indexes\n",
    " - pd.merage([df1,df2]): many joins on multiple columns\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Left & right merging on multiple columns\n",
    "You now have, in addition to the revenue and managers DataFrames from prior exercises, a DataFrame sales that summarizes units sold from specific branches (identified by city and state but not branch_id).\n",
    "\n",
    "Once again, the managers DataFrame uses the label branch in place of city as in the other two DataFrames. Your task here is to employ left and right merges to preserve data and identify where data is missing.\n",
    "\n",
    "By merging revenue and sales with a right merge, you can identify the missing revenue values. Here, you don't need to specify left_on or right_on because the columns to merge on have matching labels.\n",
    "\n",
    "By merging sales and managers with a left merge, you can identify the missing manager. Here, the columns to merge on have conflicting labels, so you must specify left_on and right_on. In both cases, you're looking to figure out how to connect the fields in rows containing Springfield.\n",
    "\n",
    "pandas has been imported as pd and the three DataFrames revenue, managers, and sales have been pre-loaded. They have been printed for you to explore in the IPython Shell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "#init\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "managers = pd.read_csv(path+'managers2.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "managers.head()\n",
    "managers.columns = managers.columns.str.strip()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "revenue = pd.read_csv(path+'revenue2.csv')\n",
    "revenue.columns = revenue.columns.str.strip()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>branch_id</th>\n",
       "      <th>city</th>\n",
       "      <th>revenue</th>\n",
       "      <th>state</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>10</td>\n",
       "      <td>Austin</td>\n",
       "      <td>100</td>\n",
       "      <td>TX</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>20</td>\n",
       "      <td>Denver</td>\n",
       "      <td>83</td>\n",
       "      <td>CO</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>30</td>\n",
       "      <td>Springfield</td>\n",
       "      <td>4</td>\n",
       "      <td>IL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>47</td>\n",
       "      <td>Mendocino</td>\n",
       "      <td>200</td>\n",
       "      <td>CA</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>branch_id</th>\n",
       "      <th>city</th>\n",
       "      <th>revenue</th>\n",
       "      <th>state</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>10</td>\n",
       "      <td>Austin</td>\n",
       "      <td>100</td>\n",
       "      <td>TX</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>20</td>\n",
       "      <td>Denver</td>\n",
       "      <td>83</td>\n",
       "      <td>CO</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>30</td>\n",
       "      <td>Springfield</td>\n",
       "      <td>4</td>\n",
       "      <td>IL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>47</td>\n",
       "      <td>Mendocino</td>\n",
       "      <td>200</td>\n",
       "      <td>CA</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "revenue.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>city</th>\n",
       "      <th>state</th>\n",
       "      <th>units</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Mendocino</td>\n",
       "      <td>CA</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Denver</td>\n",
       "      <td>CO</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Austin</td>\n",
       "      <td>TX</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Springfield</td>\n",
       "      <td>MO</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Springfield</td>\n",
       "      <td>IL</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>city</th>\n",
       "      <th>state</th>\n",
       "      <th>units</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Mendocino</td>\n",
       "      <td>CA</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Denver</td>\n",
       "      <td>CO</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Austin</td>\n",
       "      <td>TX</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Springfield</td>\n",
       "      <td>MO</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Springfield</td>\n",
       "      <td>IL</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "sales = pd.read_csv(path+'sales2.csv')\n",
    "sales.columns = sales.columns.str.strip()\n",
    "sales.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   branch_id         city  revenue state  units\n0       10.0  Austin         100.0    TX      2\n1       20.0  Denver          83.0    CO      4\n2       30.0  Springfield      4.0    IL      1\n3       47.0  Mendocino      200.0    CA      1\n4        NaN  Springfield      NaN    MO      5\n"
     ]
    }
   ],
   "source": [
    "# Merge revenue and sales: revenue_and_sales\n",
    "#Execute a right merge using pd.merge() with revenue and sales to yield a new DataFrame revenue_and_sales.\n",
    "#Use how='right' and on=['city', 'state'].\n",
    "\n",
    "revenue_and_sales = pd.merge(revenue,sales,how='right',on=['city', 'state'])\n",
    "\n",
    "# Print revenue_and_sales\n",
    "print(revenue_and_sales)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          city state  units       branch  branch_id   manager\n0  Mendocino      CA      1  Mendocino         47.0  Brett   \n1  Denver         CO      4  Denver            20.0  Joel    \n2  Austin         TX      2  Austin            10.0  Charlers\n3  Springfield    MO      5  Springfield       31.0  Sally   \n4  Springfield    IL      1          NaN        NaN       NaN\n"
     ]
    }
   ],
   "source": [
    "# Merge sales and managers: sales_and_managers\n",
    "sales_and_managers =pd.merge(sales, managers, how='left', left_on=['city', 'state'], right_on=['branch', 'state'])\n",
    "\n",
    "# Print sales_and_managers\n",
    "print(sales_and_managers)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading Olympic edition DataFrame\n",
    "In this chapter, you'll be using The Guardian's Olympic medal dataset.\n",
    "\n",
    "Your first task here is to prepare a DataFrame editions from a tab-separated values (TSV) file.\n",
    "\n",
    "Initially, editions has 26 rows (one for each Olympic edition, i.e., a year in which the Olympics was held) and 7 columns: 'Edition', 'Bronze', 'Gold', 'Silver', 'Grand Total', 'City', and 'Country'.\n",
    "\n",
    "For the analysis that follows, you won't need the overall medal counts, so you want to keep only the useful columns from editions: 'Edition', 'Grand Total', City, and Country."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Merging DataFrames with outer join\n",
    "This exercise picks up where the previous one left off. The DataFrames revenue, managers, and sales are pre-loaded into your namespace (and, of course, pandas is imported as pd). Moreover, the merged DataFrames revenue_and_sales and sales_and_managers have been pre-computed exactly as you did in the previous exercise.\n",
    "\n",
    "The merged DataFrames contain enough information to construct a DataFrame with 5 rows with all known information correctly aligned and each branch listed only once. You will try to merge the merged DataFrames on all matching keys (which computes an inner join by default). You can compare the result to an outer join and also to an outer join with restricted subset of columns as keys."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>city</th>\n",
       "      <th>state</th>\n",
       "      <th>units</th>\n",
       "      <th>branch</th>\n",
       "      <th>branch_id</th>\n",
       "      <th>manager</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Mendocino</td>\n",
       "      <td>CA</td>\n",
       "      <td>1   Mendocino</td>\n",
       "      <td>47.0</td>\n",
       "      <td>Brett</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Denver</td>\n",
       "      <td>CO</td>\n",
       "      <td>4      Denver</td>\n",
       "      <td>20.0</td>\n",
       "      <td>Joel</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Austin</td>\n",
       "      <td>TX</td>\n",
       "      <td>2      Austin</td>\n",
       "      <td>10.0</td>\n",
       "      <td>Charlers</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Springfield</td>\n",
       "      <td>MO</td>\n",
       "      <td>5 Springfield</td>\n",
       "      <td>31.0</td>\n",
       "      <td>Sally</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Springfield</td>\n",
       "      <td>IL</td>\n",
       "      <td>1         NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>city</th>\n",
       "      <th>state</th>\n",
       "      <th>units</th>\n",
       "      <th>branch</th>\n",
       "      <th>branch_id</th>\n",
       "      <th>manager</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Mendocino</td>\n",
       "      <td>CA</td>\n",
       "      <td>1   Mendocino</td>\n",
       "      <td>47.0</td>\n",
       "      <td>Brett</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Denver</td>\n",
       "      <td>CO</td>\n",
       "      <td>4      Denver</td>\n",
       "      <td>20.0</td>\n",
       "      <td>Joel</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Austin</td>\n",
       "      <td>TX</td>\n",
       "      <td>2      Austin</td>\n",
       "      <td>10.0</td>\n",
       "      <td>Charlers</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Springfield</td>\n",
       "      <td>MO</td>\n",
       "      <td>5 Springfield</td>\n",
       "      <td>31.0</td>\n",
       "      <td>Sally</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Springfield</td>\n",
       "      <td>IL</td>\n",
       "      <td>1         NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#init \n",
    "import pandas as pd\n",
    "sales_and_managers = pd.read_csv(path+'sales_and_managers.csv')\n",
    "sales_and_managers.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>branch_id</th>\n",
       "      <th>city</th>\n",
       "      <th>revenue</th>\n",
       "      <th>state</th>\n",
       "      <th>units</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>10.0</td>\n",
       "      <td>Austin</td>\n",
       "      <td>100.0</td>\n",
       "      <td>TX</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>20.0</td>\n",
       "      <td>Denver</td>\n",
       "      <td>83.0</td>\n",
       "      <td>CO</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>30.0</td>\n",
       "      <td>Springfield</td>\n",
       "      <td>4.0</td>\n",
       "      <td>IL</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>47.0</td>\n",
       "      <td>Mendocino</td>\n",
       "      <td>200.0</td>\n",
       "      <td>CA</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>NaN</td>\n",
       "      <td>Springfield</td>\n",
       "      <td>NaN</td>\n",
       "      <td>MO</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>branch_id</th>\n",
       "      <th>city</th>\n",
       "      <th>revenue</th>\n",
       "      <th>state</th>\n",
       "      <th>units</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>10.0</td>\n",
       "      <td>Austin</td>\n",
       "      <td>100.0</td>\n",
       "      <td>TX</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>20.0</td>\n",
       "      <td>Denver</td>\n",
       "      <td>83.0</td>\n",
       "      <td>CO</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>30.0</td>\n",
       "      <td>Springfield</td>\n",
       "      <td>4.0</td>\n",
       "      <td>IL</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>47.0</td>\n",
       "      <td>Mendocino</td>\n",
       "      <td>200.0</td>\n",
       "      <td>CA</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>NaN</td>\n",
       "      <td>Springfield</td>\n",
       "      <td>NaN</td>\n",
       "      <td>MO</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "revenue_and_sales = pd.read_csv(path+'revenue_and_sales.csv')\n",
    "revenue_and_sales.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Empty DataFrame\nColumns: [city       , state, units        , branch, branch_id, manager, revenue, units]\nIndex: []\n   city         state  units          branch  branch_id  manager  revenue  \\\n0  Mendocino    CA     1   Mendocino  47.0        Brett      NaN      NaN   \n1  Denver       CO     4      Denver  20.0         Joel      NaN      NaN   \n2  Austin       TX     2      Austin  10.0     Charlers      NaN      NaN   \n3  Springfield  MO     5 Springfield  31.0        Sally      NaN      NaN   \n4  Springfield  IL     1         NaN  NaN           NaN      NaN      NaN   \n5  Austin       TX               NaN     NaN  10.0           NaN  100.0     \n6  Denver       CO               NaN     NaN  20.0           NaN  83.0      \n7  Springfield  IL               NaN     NaN  30.0           NaN  4.0       \n8  Mendocino    CA               NaN     NaN  47.0           NaN  200.0     \n9  Springfield  MO               NaN     NaN  NaN            NaN  NaN       \n\n   units  \n0    NaN  \n1    NaN  \n2    NaN  \n3    NaN  \n4    NaN  \n5    2.0  \n6    4.0  \n7    1.0  \n8    1.0  \n9    5.0  \n  city       _x  state  units          branch branch_id_x  manager  \\\n0   Mendocino    CA     1   Mendocino  47.0         Brett      NaN   \n1   Denver       CO     4      Denver  20.0          Joel      NaN   \n2   Austin       TX     2      Austin  10.0      Charlers      NaN   \n3   Springfield  MO     5 Springfield  31.0         Sally      NaN   \n4   Springfield  IL     1         NaN  NaN            NaN      NaN   \n\n  branch_id_y city       _y  revenue  units  \n0   47.0        Mendocino    200.0        1  \n1   20.0        Denver       83.0         4  \n2   10.0        Austin       100.0        2  \n3   NaN         Springfield  NaN          5  \n4   30.0        Springfield  4.0          1  \n"
     ]
    }
   ],
   "source": [
    "# Perform the first merge: merge_default\n",
    "merge_default = pd.merge(sales_and_managers,revenue_and_sales)\n",
    "\n",
    "# Print merge_default\n",
    "print(merge_default)\n",
    "\n",
    "# Perform the second merge: merge_outer\n",
    "merge_outer = pd.merge(sales_and_managers,revenue_and_sales,how='outer')\n",
    "\n",
    "# Print merge_outer\n",
    "print(merge_outer)\n",
    "\n",
    "# Perform the third merge: merge_outer_on\n",
    "merge_outer_on =pd.merge(sales_and_managers,revenue_and_sales,on=['state'],how='outer')\n",
    "\n",
    "# Print merge_outer_on\n",
    "print(merge_outer_on)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##Using merge_ordered() \n",
    "This exercise uses pre-loaded DataFrames austin and houston that contain weather data from the cities Austin and Houston respectively. They have been printed in the IPython Shell for you to examine.\n",
    "\n",
    "Weather conditions were recorded on separate days and you need to merge these two DataFrames together such that the dates are ordered. To do this, you'll use pd.merge_ordered(). After you're done, note the order of the rows before and after merging."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "#init\n",
    "houston= pd.read_csv(path+'huston.csv',parse_dates=True)\n",
    "austin = pd.read_csv(path+'austin.csv',parse_dates=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         date  ratings\n0  2016-01-01   Cloudy\n1  2016-01-01   Cloudy\n2  2016-01-04    Rainy\n3  2016-01-17    Sunny\n4  2016-02-08   Cloudy\n5  2016-03-01    Sunny\n         date ratings_aus ratings_hus\n0  2016-01-01      Cloudy      Cloudy\n1  2016-01-04         NaN       Rainy\n2  2016-01-17       Sunny         NaN\n3  2016-02-08      Cloudy         NaN\n4  2016-03-01         NaN       Sunny\n         date ratings_aus ratings_hus\n0  2016-01-01      Cloudy      Cloudy\n1  2016-01-04      Cloudy       Rainy\n2  2016-01-17       Sunny       Rainy\n3  2016-02-08      Cloudy       Rainy\n4  2016-03-01      Cloudy       Sunny\n"
     ]
    }
   ],
   "source": [
    "# Perform the first ordered merge: tx_weather\n",
    "tx_weather = pd.merge_ordered(austin,houston)\n",
    "\n",
    "# Print tx_weather\n",
    "print(tx_weather)\n",
    "\n",
    "# Perform the second ordered merge: tx_weather_suff\n",
    "tx_weather_suff = pd.merge_ordered(austin,houston,on='date',suffixes=['_aus','_hus'])\n",
    "\n",
    "# Print tx_weather_suff\n",
    "print(tx_weather_suff)\n",
    "\n",
    "# Perform the third ordered merge: tx_weather_ffill\n",
    "tx_weather_ffill = pd.merge_ordered(austin,houston,on='date',suffixes=['_aus','_hus'],fill_method='ffill')\n",
    "\n",
    "# Print tx_weather_ffill\n",
    "print(tx_weather_ffill)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using merge_asof()\n",
    "Similar to pd.merge_ordered(), the pd.merge_asof() function will also merge values in order using the on column, but for each row in the left DataFrame, only rows from the right DataFrame whose 'on' column values are less than the left value will be kept.\n",
    "\n",
    "This function can be use to align disparate datetime frequencies without having to first resample.\n",
    "\n",
    "Here, you'll merge monthly oil prices (US dollars) into a full automobile fuel efficiency dataset. The oil and automobile DataFrames have been pre-loaded as oil and auto. The first 5 rows of each have been printed in the IPython Shell for you to explore.\n",
    "\n",
    "These datasets will align such that the first price of the year will be broadcast into the rows of the automobiles DataFrame. This is considered correct since by the start of any given year, most automobiles for that year will have already been manufactured.\n",
    "\n",
    "You'll then inspect the merged DataFrame, resample by year and compute the mean 'Price' and 'mpg'. You should be able to see a trend in these two columns, that you can confirm by computing the Pearson correlation between resampled 'Price' and 'mpg'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "# int\n",
    "oil = pd.read_csv(path+'oil.csv',parse_dates=True)\n",
    "auto = pd.read_csv(path+'auto.csv',parse_dates=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>Price</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1970-01-01</td>\n",
       "      <td>3.35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1970-02-01</td>\n",
       "      <td>3.35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1970-03-01</td>\n",
       "      <td>3.35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1970-04-01</td>\n",
       "      <td>3.35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1970-05-01</td>\n",
       "      <td>3.35</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>Price</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1970-01-01</td>\n",
       "      <td>3.35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1970-02-01</td>\n",
       "      <td>3.35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1970-03-01</td>\n",
       "      <td>3.35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1970-04-01</td>\n",
       "      <td>3.35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1970-05-01</td>\n",
       "      <td>3.35</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "oil.head()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mpg</th>\n",
       "      <th>cyl</th>\n",
       "      <th>displ</th>\n",
       "      <th>hp</th>\n",
       "      <th>weight</th>\n",
       "      <th>accel</th>\n",
       "      <th>yr</th>\n",
       "      <th>origin</th>\n",
       "      <th>name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>18.0</td>\n",
       "      <td>8</td>\n",
       "      <td>307.0</td>\n",
       "      <td>130</td>\n",
       "      <td>3504</td>\n",
       "      <td>12.0</td>\n",
       "      <td>1970-01-01</td>\n",
       "      <td>US</td>\n",
       "      <td>chevrolet chevelle malibu</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>15.0</td>\n",
       "      <td>8</td>\n",
       "      <td>350.0</td>\n",
       "      <td>165</td>\n",
       "      <td>3693</td>\n",
       "      <td>11.5</td>\n",
       "      <td>1970-01-01</td>\n",
       "      <td>US</td>\n",
       "      <td>buick skylark 320</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>18.0</td>\n",
       "      <td>8</td>\n",
       "      <td>318.0</td>\n",
       "      <td>150</td>\n",
       "      <td>3436</td>\n",
       "      <td>11.0</td>\n",
       "      <td>1970-01-01</td>\n",
       "      <td>US</td>\n",
       "      <td>plymouth satellite</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>16.0</td>\n",
       "      <td>8</td>\n",
       "      <td>304.0</td>\n",
       "      <td>150</td>\n",
       "      <td>3433</td>\n",
       "      <td>12.0</td>\n",
       "      <td>1970-01-01</td>\n",
       "      <td>US</td>\n",
       "      <td>amc rebel sst</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>17.0</td>\n",
       "      <td>8</td>\n",
       "      <td>302.0</td>\n",
       "      <td>140</td>\n",
       "      <td>3449</td>\n",
       "      <td>10.5</td>\n",
       "      <td>1970-01-01</td>\n",
       "      <td>US</td>\n",
       "      <td>ford torino</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mpg</th>\n",
       "      <th>cyl</th>\n",
       "      <th>displ</th>\n",
       "      <th>hp</th>\n",
       "      <th>weight</th>\n",
       "      <th>accel</th>\n",
       "      <th>yr</th>\n",
       "      <th>origin</th>\n",
       "      <th>name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>18.0</td>\n",
       "      <td>8</td>\n",
       "      <td>307.0</td>\n",
       "      <td>130</td>\n",
       "      <td>3504</td>\n",
       "      <td>12.0</td>\n",
       "      <td>1970-01-01</td>\n",
       "      <td>US</td>\n",
       "      <td>chevrolet chevelle malibu</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>15.0</td>\n",
       "      <td>8</td>\n",
       "      <td>350.0</td>\n",
       "      <td>165</td>\n",
       "      <td>3693</td>\n",
       "      <td>11.5</td>\n",
       "      <td>1970-01-01</td>\n",
       "      <td>US</td>\n",
       "      <td>buick skylark 320</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>18.0</td>\n",
       "      <td>8</td>\n",
       "      <td>318.0</td>\n",
       "      <td>150</td>\n",
       "      <td>3436</td>\n",
       "      <td>11.0</td>\n",
       "      <td>1970-01-01</td>\n",
       "      <td>US</td>\n",
       "      <td>plymouth satellite</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>16.0</td>\n",
       "      <td>8</td>\n",
       "      <td>304.0</td>\n",
       "      <td>150</td>\n",
       "      <td>3433</td>\n",
       "      <td>12.0</td>\n",
       "      <td>1970-01-01</td>\n",
       "      <td>US</td>\n",
       "      <td>amc rebel sst</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>17.0</td>\n",
       "      <td>8</td>\n",
       "      <td>302.0</td>\n",
       "      <td>140</td>\n",
       "      <td>3449</td>\n",
       "      <td>10.5</td>\n",
       "      <td>1970-01-01</td>\n",
       "      <td>US</td>\n",
       "      <td>ford torino</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "auto.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'NoneType' object is not callable",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-148-0a1cbed23bc2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Merge auto and oil: merged\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m#Merge auto and oil using pd.merge_asof() with left_on='yr' and right_on='Date'. Store the result as merged.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mmerged\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmerge_asof\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mauto\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0moil\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mleft_on\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'yr'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mright_on\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'Date'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# Print the tail of merged\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/TensorflowCPUPY35/lib/python3.5/site-packages/pandas/core/reshape/merge.py\u001b[0m in \u001b[0;36mmerge_asof\u001b[0;34m(left, right, on, left_on, right_on, left_index, right_index, by, left_by, right_by, suffixes, tolerance, allow_exact_matches, direction)\u001b[0m\n\u001b[1;32m    491\u001b[0m                     \u001b[0mallow_exact_matches\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mallow_exact_matches\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    492\u001b[0m                     direction=direction)\n\u001b[0;32m--> 493\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mop\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_result\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    494\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    495\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/TensorflowCPUPY35/lib/python3.5/site-packages/pandas/core/reshape/merge.py\u001b[0m in \u001b[0;36mget_result\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1085\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1086\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget_result\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1087\u001b[0;31m         \u001b[0mjoin_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mleft_indexer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mright_indexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_join_info\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1088\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1089\u001b[0m         \u001b[0;31m# this is a bit kludgy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/TensorflowCPUPY35/lib/python3.5/site-packages/pandas/core/reshape/merge.py\u001b[0m in \u001b[0;36m_get_join_info\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    746\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    747\u001b[0m             (left_indexer,\n\u001b[0;32m--> 748\u001b[0;31m              right_indexer) = self._get_join_indexers()\n\u001b[0m\u001b[1;32m    749\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    750\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mright_index\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/TensorflowCPUPY35/lib/python3.5/site-packages/pandas/core/reshape/merge.py\u001b[0m in \u001b[0;36m_get_join_indexers\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1367\u001b[0m                         \u001b[0mright_values\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1368\u001b[0m                         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mallow_exact_matches\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1369\u001b[0;31m                         tolerance)\n\u001b[0m\u001b[1;32m   1370\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1371\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: 'NoneType' object is not callable"
     ],
     "output_type": "error"
    }
   ],
   "source": [
    "# Merge auto and oil: merged\n",
    "#Merge auto and oil using pd.merge_asof() with left_on='yr' and right_on='Date'. Store the result as merged.\n",
    "merged = pd.merge_asof(auto,oil,left_on='yr',right_on='Date')\n",
    "\n",
    "# Print the tail of merged\n",
    "print(merged.tail())\n",
    "\n",
    "# Resample merged: yearly\n",
    "yearly = merged.resample('A',on='Date')[['mpg','Price']].mean()\n",
    "\n",
    "# Print yearly\n",
    "print(yearly)\n",
    "\n",
    "# print yearly.corr()\n",
    "print(yearly.corr())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading Olympic edition DataFrame\n",
    "In this chapter, you'll be using The Guardian's Olympic medal dataset.\n",
    "\n",
    "Your first task here is to prepare a DataFrame editions from a tab-separated values (TSV) file.\n",
    "\n",
    "Initially, editions has 26 rows (one for each Olympic edition, i.e., a year in which the Olympics was held) and 7 columns: 'Edition', 'Bronze', 'Gold', 'Silver', 'Grand Total', 'City', and 'Country'.\n",
    "\n",
    "For the analysis that follows, you won't need the overall medal counts, so you want to keep only the useful columns from editions: 'Edition', 'Grand Total', City, and Country."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/victor/Documents/Dev/TensorflowPY36CPU/_1_PythonBasic/Pandas/Summer Olympic medallists 1896 to 2008 - EDITIONS.tsv\n"
     ]
    }
   ],
   "source": [
    "file_path = 'Summer Olympic medallists 1896 to 2008 - EDITIONS.tsv'\n",
    "print(path + file_path)\n",
    "\n",
    "filename ='/Users/victor/Documents/Dev/TensorflowPY36CPU/_1_PythonBasic/Pandas/Summer Olympic medalists 1896 to 2008 - EDITIONS.tsv'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Edition  Grand Total         City                     Country\n0      1896          151       Athens                      Greece\n1      1900          512        Paris                      France\n2      1904          470    St. Louis               United States\n3      1908          804       London              United Kingdom\n4      1912          885    Stockholm                      Sweden\n5      1920         1298      Antwerp                     Belgium\n6      1924          884        Paris                      France\n7      1928          710    Amsterdam                 Netherlands\n8      1932          615  Los Angeles               United States\n9      1936          875       Berlin                     Germany\n10     1948          814       London              United Kingdom\n11     1952          889     Helsinki                     Finland\n12     1956          885    Melbourne                   Australia\n13     1960          882         Rome                       Italy\n14     1964         1010        Tokyo                       Japan\n15     1968         1031  Mexico City                      Mexico\n16     1972         1185       Munich  West Germany (now Germany)\n17     1976         1305     Montreal                      Canada\n18     1980         1387       Moscow       U.S.S.R. (now Russia)\n19     1984         1459  Los Angeles               United States\n20     1988         1546        Seoul                 South Korea\n21     1992         1705    Barcelona                       Spain\n22     1996         1859      Atlanta               United States\n23     2000         2015       Sydney                   Australia\n24     2004         1998       Athens                      Greece\n25     2008         2042      Beijing                       China\n"
     ]
    }
   ],
   "source": [
    "#Import pandas\n",
    "import pandas as pd\n",
    "\n",
    "# Create file path: file_path\n",
    "file_path = 'Summer Olympic medallists 1896 to 2008 - EDITIONS.tsv'\n",
    "\n",
    "# Load DataFrame from file_path: editions\n",
    "editions = pd.read_csv(path+file_path,sep='\\t')\n",
    "\n",
    "# Extract the relevant columns: editions\n",
    "editions = editions[['Edition', 'Grand Total', 'City', 'Country']]\n",
    "\n",
    "# Print editions DataFrame\n",
    "print(editions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading IOC codes DataFrame\n",
    "Your task here is to prepare a DataFrame ioc_codes from a comma-separated values (CSV) file.\n",
    "\n",
    "Initially, ioc_codes has 200 rows (one for each country) and 3 columns: 'Country', 'NOC', & 'ISO code'.\n",
    "\n",
    "For the analysis that follows, you want to keep only the useful columns from ioc_codes: 'Country' and 'NOC' (the column 'NOC' contains three-letter codes representing each country)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           Country  NOC\n0      Afghanistan  AFG\n1          Albania  ALB\n2          Algeria  ALG\n3  American Samoa*  ASA\n4          Andorra  AND\n             Country  NOC\n196          Vietnam  VIE\n197  Virgin Islands*  ISV\n198            Yemen  YEM\n199           Zambia  ZAM\n200         Zimbabwe  ZIM\n"
     ]
    }
   ],
   "source": [
    "# Import pandas\n",
    "import pandas as pd\n",
    "\n",
    "# Create the file path: file_path\n",
    "file_path = 'Summer Olympic medallists 1896 to 2008 - IOC COUNTRY CODES.csv'\n",
    "\n",
    "# Load DataFrame from file_path: ioc_codes\n",
    "ioc_codes = pd.read_csv(path+ file_path)\n",
    "\n",
    "# Extract the relevant columns: ioc_codes\n",
    "ioc_codes = ioc_codes[['Country','NOC']]\n",
    "\n",
    "# Print first and last 5 rows of ioc_codes\n",
    "print(ioc_codes.head())\n",
    "print(ioc_codes.tail())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Building medals DataFrame\n",
    "Here, you'll start with the DataFrame editions from the previous exercise.\n",
    "\n",
    "You have a sequence of files summer_1896.csv, summer_1900.csv, ..., summer_2008.csv, one for each Olympic edition (year).\n",
    "\n",
    "You will build up a dictionary medals_dict with the Olympic editions (years) as keys and DataFrames as values.\n",
    "\n",
    "The dictionary is built up inside a loop over the year of each Olympic edition (from the Index of editions).\n",
    "\n",
    "Once the dictionary of DataFrames is built up, you will combine the DataFrames using pd.concat()."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Computing percentage change in fraction of medals won\n",
    "Here, you'll start with the DataFrames editions, medals, medal_counts, & fractions from prior exercises.\n",
    "\n",
    "To see if there is a host country advantage, you first want to see how the fraction of medals won changes from edition to edition.\n",
    "\n",
    "The expanding mean provides a way to see this down each column. It is the value of the mean with all the data available up to that point in time. If you are interested in learning more about pandas' expanding transformations, this section of the pandas documentation has additional information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "File b'/Users/victor/Documents/Dev/TensorflowPY36CPU/_1_PythonBasic/Pandas/summer_1896.csv' does not exist",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-152-2b549519f163>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0;31m# Load file_path into a DataFrame: medals_dict[year]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m     \u001b[0mmedals_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0myear\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mfile_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0;31m# Extract relevant columns: medals_dict[year]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/TensorflowCPUPY35/lib/python3.5/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36mparser_f\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, escapechar, comment, encoding, dialect, tupleize_cols, error_bad_lines, warn_bad_lines, skipfooter, skip_footer, doublequote, delim_whitespace, as_recarray, compact_ints, use_unsigned, low_memory, buffer_lines, memory_map, float_precision)\u001b[0m\n\u001b[1;32m    707\u001b[0m                     skip_blank_lines=skip_blank_lines)\n\u001b[1;32m    708\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 709\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    710\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    711\u001b[0m     \u001b[0mparser_f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/TensorflowCPUPY35/lib/python3.5/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    447\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    448\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 449\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    450\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    451\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/TensorflowCPUPY35/lib/python3.5/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m    816\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'has_index_names'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'has_index_names'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    817\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 818\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    819\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    820\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/TensorflowCPUPY35/lib/python3.5/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, engine)\u001b[0m\n\u001b[1;32m   1047\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'c'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1048\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'c'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1049\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCParserWrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1050\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1051\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'python'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/TensorflowCPUPY35/lib/python3.5/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, src, **kwds)\u001b[0m\n\u001b[1;32m   1693\u001b[0m         \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'allow_leading_cols'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex_col\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1694\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1695\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparsers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTextReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1696\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1697\u001b[0m         \u001b[0;31m# XXX\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.__cinit__\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._setup_parser_source\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: File b'/Users/victor/Documents/Dev/TensorflowPY36CPU/_1_PythonBasic/Pandas/summer_1896.csv' does not exist"
     ],
     "output_type": "error"
    }
   ],
   "source": [
    "# Import pandas\n",
    "import pandas as pd\n",
    "\n",
    "# Create empty dictionary: medals_dict\n",
    "medals_dict = {}\n",
    "\n",
    "for year in editions['Edition']:\n",
    "\n",
    "    # Create the file path: file_path\n",
    "    file_path = 'summer_{:d}.csv'.format(year)\n",
    "    \n",
    "    # Load file_path into a DataFrame: medals_dict[year]\n",
    "    medals_dict[year] = pd.read_csv(path+file_path)\n",
    "    \n",
    "    # Extract relevant columns: medals_dict[year]\n",
    "    medals_dict[year] = medals_dict[['Athlete','NOC','Medal']]\n",
    "    \n",
    "    # Assign year to column 'Edition' of medals_dict\n",
    "    medals_dict[year]['Edition'] = year\n",
    "    \n",
    "# Concatenate medals_dict: medals\n",
    "medals = pd.concat(medals_dict,ignore_index=True)\n",
    "\n",
    "# Print first and last 5 rows of medals\n",
    "print(medals.head())\n",
    "print(medals.tail())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Counting medals by country/edition in a pivot table\n",
    "Here, you'll start with the concatenated DataFrame medals from the previous exercise.\n",
    "\n",
    "You can construct a pivot table to see the number of medals each country won in each year. The result is a new DataFrame with the Olympic edition on the Index and with 138 country NOC codes as columns. If you want a refresher on pivot tables, it may be useful to refer back to the relevant exercises in Manipulating DataFrames with pandas.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'Athlete'",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-153-f7f4c393a1f3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Construct the pivot_table: medal_counts\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m#counted = medals.pivot_table(index='NOC', values='Athlete', columns='Medal', aggfunc='count')\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mmedal_counts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmedals\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpivot_table\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'Edition'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'Athlete'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'NOC'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0maggfunc\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'count'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# Print the first & last 5 rows of medal_counts\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/TensorflowCPUPY35/lib/python3.5/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36mpivot_table\u001b[0;34m(self, values, index, columns, aggfunc, fill_value, margins, dropna, margins_name)\u001b[0m\n\u001b[1;32m   4466\u001b[0m                            \u001b[0maggfunc\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maggfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfill_value\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfill_value\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4467\u001b[0m                            \u001b[0mmargins\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmargins\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdropna\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdropna\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4468\u001b[0;31m                            margins_name=margins_name)\n\u001b[0m\u001b[1;32m   4469\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4470\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlevel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdropna\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/TensorflowCPUPY35/lib/python3.5/site-packages/pandas/core/reshape/pivot.py\u001b[0m in \u001b[0;36mpivot_table\u001b[0;34m(data, values, index, columns, aggfunc, fill_value, margins, dropna, margins_name)\u001b[0m\n\u001b[1;32m     56\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mvalues\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 58\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     59\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m         \u001b[0mto_filter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'Athlete'"
     ],
     "output_type": "error"
    }
   ],
   "source": [
    "# Construct the pivot_table: medal_counts\n",
    "#counted = medals.pivot_table(index='NOC', values='Athlete', columns='Medal', aggfunc='count')\n",
    "medal_counts = medals.pivot_table(index='Edition',values='Athlete',columns='NOC',aggfunc='count')\n",
    "\n",
    "# Print the first & last 5 rows of medal_counts\n",
    "print(medal_counts.head())\n",
    "print(medal_counts.tail())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Computing fraction of medals per Olympic edition\n",
    "In this exercise, you'll start with the DataFrames editions, medals, & medal_counts from prior exercises.\n",
    "\n",
    "You can extract a Series with the total number of medals awarded in each Olympic edition.\n",
    "\n",
    "The DataFrame medal_counts can be divided row-wise by the total number of medals awarded each edition; the method .divide() performs the broadcast as you require.\n",
    "\n",
    "This gives you a normalized indication of each country's performance in each edition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "import base64\n",
    "\n",
    "from IPython.display import HTML\n",
    "\n",
    "def create_download_link( df, title = \"Download CSV file\", filename = \"data.csv\"):  \n",
    "    csv = df.to_csv()\n",
    "    b64 = base64.b64encode(csv.encode())\n",
    "    payload = b64.decode()\n",
    "    html = '<a download=\"{filename}\" href=\"data:text/csv;base64,{payload}\" target=\"_blank\">{title}</a>'\n",
    "    html = html.format(payload=payload,title=title,filename=filename)\n",
    "    return HTML(html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<a download=\"data.csv\" href=\"data:text/csv;base64,,bronze,bronze,bronze,silver,silver,silver,gold,gold,gold
,NOC,Country,Total,NOC,Country,Total,NOC,Country,Total
0,USA,United States,1052.0,USA,United States,1195.0,USA,United States,2088.0
1,URS,Soviet Union,584.0,URS,Soviet Union,627.0,URS,Soviet Union,838.0
2,GBR,United Kingdom,505.0,GBR,United Kingdom,591.0,GBR,United Kingdom,498.0
3,FRA,France,475.0,FRA,France,461.0,FRA,France,378.0
4,GER,Germany,454.0,GER,Germany,350.0,GER,Germany,407.0
5,AUS,Australia,413.0,AUS,Australia,369.0,AUS,Australia,293.0
6,ITA,Italy,374.0,ITA,Italy,394.0,ITA,Italy,460.0
7,HUN,Hungary,345.0,HUN,Hungary,308.0,HUN,Hungary,400.0
8,SWE,Sweden,325.0,SWE,Sweden,349.0,SWE,Sweden,347.0
9,NED,Netherlands,320.0,NED,Netherlands,250.0,NED,Netherlands,212.0
10,ROU,Romania,282.0,ROU,Romania,187.0,ROU,Romania,155.0
11,JPN,Japan,270.0,JPN,Japan,228.0,JPN,Japan,206.0
12,RUS,Russia,240.0,RUS,Russia,206.0,RUS,Russia,192.0
13,CAN,Canada,227.0,CAN,Canada,211.0,CAN,Canada,154.0
14,GDR,East Germany,225.0,GDR,East Germany,271.0,GDR,East Germany,329.0
15,POL,Poland,223.0,POL,Poland,173.0,POL,Poland,103.0
16,FIN,Finland,209.0,FIN,Finland,118.0,FIN,Finland,124.0
17,CHN,China,193.0,CHN,China,252.0,CHN,China,234.0
18,FRG,West Germany,180.0,FRG,West Germany,167.0,FRG,West Germany,143.0
19,BRA,Brazil,174.0,BRA,Brazil,139.0,BRA,Brazil,59.0
20,DEN,Denmark,152.0,DEN,Denmark,192.0,DEN,Denmark,147.0
21,BEL,Belgium,150.0,BEL,Belgium,167.0,BEL,Belgium,91.0
22,NOR,Norway,144.0,NOR,Norway,199.0,NOR,Norway,194.0
23,SUI,Switzerland,138.0,SUI,Switzerland,165.0,SUI,Switzerland,73.0
24,BUL,Bulgaria,136.0,BUL,Bulgaria,142.0,BUL,Bulgaria,53.0
25,KOR,"Korea, South",135.0,KOR,"Korea, South",191.0,KOR,"Korea, South",140.0
26,YUG,Yugoslavia,118.0,YUG,Yugoslavia,174.0,YUG,Yugoslavia,143.0
27,CUB,Cuba,109.0,CUB,Cuba,126.0,CUB,Cuba,160.0
28,TCH,Czechoslovakia,105.0,TCH,Czechoslovakia,144.0,TCH,Czechoslovakia,80.0
29,ESP,Spain,92.0,ESP,Spain,193.0,ESP,Spain,92.0
30,EUA,Unified team of Germany,92.0,EUA,Unified team of Germany,100.0,EUA,Unified team of Germany,68.0
31,ARG,Argentina,88.0,ARG,Argentina,83.0,ARG,Argentina,68.0
32,UKR,Ukraine,78.0,UKR,Ukraine,38.0,UKR,Ukraine,32.0
33,EUN,Unified team,66.0,EUN,Unified team,65.0,EUN,Unified team,92.0
34,NZL,New Zealand,61.0,NZL,New Zealand,27.0,NZL,New Zealand,76.0
35,BLR,Belarus,53.0,BLR,Belarus,25.0,BLR,Belarus,14.0
36,GRE,Greece,49.0,GRE,Greece,62.0,GRE,Greece,34.0
37,MEX,Mexico,48.0,MEX,Mexico,18.0,MEX,Mexico,14.0
38,AUT,Austria,44.0,AUT,Austria,81.0,AUT,Austria,21.0
39,LTU,Lithuania,42.0,LTU,Lithuania,4.0,LTU,Lithuania,4.0
40,JAM,Jamaica,39.0,JAM,Jamaica,40.0,JAM,Jamaica,23.0
41,RSA,South Africa,36.0,RSA,South Africa,37.0,RSA,South Africa,24.0
42,PAK,Pakistan,34.0,PAK,Pakistan,45.0,PAK,Pakistan,42.0
43,IND,India,34.0,IND,India,16.0,IND,India,128.0
44,PRK,"Korea, North",30.0,PRK,"Korea, North",12.0,PRK,"Korea, North",10.0
45,URU,Uruguay,30.0,URU,Uruguay,2.0,URU,Uruguay,44.0
46,NGR,Nigeria,27.0,NGR,Nigeria,38.0,NGR,Nigeria,19.0
47,KEN,Kenya,24.0,KEN,Kenya,31.0,KEN,Kenya,26.0
48,TUR,Turkey,22.0,TUR,Turkey,23.0,TUR,Turkey,37.0
49,IRI,Iran,22.0,IRI,Iran,15.0,IRI,Iran,11.0
50,CHI,Chile,21.0,CHI,Chile,9.0,CHI,Chile,3.0
51,EST,Estonia,19.0,EST,Estonia,9.0,EST,Estonia,9.0
52,CRO,Croatia,18.0,CRO,Croatia,30.0,CRO,Croatia,31.0
53,POR,Portugal,18.0,POR,Portugal,9.0,POR,Portugal,4.0
54,GHA,Ghana,15.0,GHA,Ghana,1.0,GHA,Ghana,
55,KAZ,Kazakhstan,14.0,KAZ,Kazakhstan,16.0,KAZ,Kazakhstan,9.0
56,ETH,Ethiopia,14.0,ETH,Ethiopia,6.0,ETH,Ethiopia,18.0
57,SRB,Serbia,14.0,SRB,Serbia,1.0,SRB,Serbia,
58,TPE,Taiwan,13.0,TPE,Taiwan,27.0,TPE,Taiwan,2.0
59,CZE,Czech Republic,13.0,CZE,Czech Republic,18.0,CZE,Czech Republic,10.0
60,INA,Indonesia,12.0,INA,Indonesia,14.0,INA,Indonesia,9.0
61,TRI,Trinidad and Tobago,11.0,TRI,Trinidad and Tobago,8.0,TRI,Trinidad and Tobago,1.0
62,SLO,Slovenia,11.0,SLO,Slovenia,6.0,SLO,Slovenia,4.0
63,GEO,Georgia,11.0,GEO,Georgia,2.0,GEO,Georgia,5.0
64,ZZX,Mixed teams,10.0,ZZX,Mixed teams,15.0,ZZX,Mixed teams,23.0
65,EGY,Egypt,10.0,EGY,Egypt,7.0,EGY,Egypt,7.0
66,MGL,Mongolia,10.0,MGL,Mongolia,7.0,MGL,Mongolia,2.0
67,MAR,Morocco,10.0,MAR,Morocco,5.0,MAR,Morocco,6.0
68,THA,Thailand,10.0,THA,Thailand,4.0,THA,Thailand,7.0
69,RU1,0,9.0,RU1,0,7.0,RU1,0,1.0
70,AZE,Azerbaijan,9.0,AZE,Azerbaijan,3.0,AZE,Azerbaijan,4.0
71,SVK,Slovakia,8.0,SVK,Slovakia,11.0,SVK,Slovakia,10.0
72,IRL,Ireland,8.0,IRL,Ireland,8.0,IRL,Ireland,8.0
73,UZB,Uzbekistan,8.0,UZB,Uzbekistan,5.0,UZB,Uzbekistan,4.0
74,ALG,Algeria,8.0,ALG,Algeria,2.0,ALG,Algeria,4.0
75,VEN,Venezuela,8.0,VEN,Venezuela,2.0,VEN,Venezuela,1.0
76,COL,Colombia,7.0,COL,Colombia,3.0,COL,Colombia,1.0
77,PHI,Philippines,7.0,PHI,Philippines,2.0,PHI,Philippines,
78,ARM,Armenia,7.0,ARM,Armenia,1.0,ARM,Armenia,1.0
79,HAI,Haiti,7.0,HAI,Haiti,1.0,HAI,Haiti,
80,BOH,Bohemia,6.0,BOH,Bohemia,1.0,BOH,Bohemia,
81,BAH,Bahamas,5.0,BAH,Bahamas,9.0,BAH,Bahamas,9.0
82,ANZ,Australasia,5.0,ANZ,Australasia,4.0,ANZ,Australasia,20.0
83,ISR,Israel,5.0,ISR,Israel,1.0,ISR,Israel,1.0
84,PUR,Puerto Rico*,5.0,PUR,Puerto Rico*,1.0,PUR,Puerto Rico*,
85,BWI,British West Indies,5.0,BWI,British West Indies,,BWI,British West Indies,
86,LAT,Latvia,4.0,LAT,Latvia,11.0,LAT,Latvia,2.0
87,MAS,Malaysia,3.0,MAS,Malaysia,3.0,MAS,Malaysia,
88,MDA,Moldova,3.0,MDA,Moldova,3.0,MDA,Moldova,
89,TUN,Tunisia,3.0,TUN,Tunisia,2.0,TUN,Tunisia,2.0
90,ISL,Iceland,2.0,ISL,Iceland,15.0,ISL,Iceland,
91,UGA,Uganda,2.0,UGA,Uganda,3.0,UGA,Uganda,1.0
92,LIB,Lebanon,2.0,LIB,Lebanon,2.0,LIB,Lebanon,
93,CRC,Costa Rica,2.0,CRC,Costa Rica,1.0,CRC,Costa Rica,1.0
94,IOP,Independent Olympic Participants (1992),2.0,IOP,Independent Olympic Participants (1992),1.0,IOP,Independent Olympic Participants (1992),
95,KGZ,Kyrgyzstan,2.0,KGZ,Kyrgyzstan,1.0,KGZ,Kyrgyzstan,
96,PAN,Panama,2.0,PAN,Panama,,PAN,Panama,1.0
97,QAT,Qatar,2.0,QAT,Qatar,,QAT,Qatar,
98,ZIM,Zimbabwe,1.0,ZIM,Zimbabwe,4.0,ZIM,Zimbabwe,18.0
99,CMR,Cameroon,1.0,CMR,Cameroon,1.0,CMR,Cameroon,20.0
100,DOM,Dominican Republic,1.0,DOM,Dominican Republic,1.0,DOM,Dominican Republic,2.0
101,SYR,Syria,1.0,SYR,Syria,1.0,SYR,Syria,1.0
102,KSA,Saudi Arabia,1.0,KSA,Saudi Arabia,1.0,KSA,Saudi Arabia,
103,TJK,Tajikistan,1.0,TJK,Tajikistan,1.0,TJK,Tajikistan,
104,ZAM,Zambia,1.0,ZAM,Zambia,1.0,ZAM,Zambia,
105,MOZ,Mozambique,1.0,MOZ,Mozambique,,MOZ,Mozambique,1.0
106,SUR,Suriname,1.0,SUR,Suriname,,SUR,Suriname,1.0
107,AFG,Afghanistan,1.0,AFG,Afghanistan,,AFG,Afghanistan,
108,BAR,Barbados,1.0,BAR,Barbados,,BAR,Barbados,
109,BER,Bermuda*,1.0,BER,Bermuda*,,BER,Bermuda*,
110,DJI,Djibouti,1.0,DJI,Djibouti,,DJI,Djibouti,
111,ERI,Eritrea,1.0,ERI,Eritrea,,ERI,Eritrea,
112,GUY,Guyana,1.0,GUY,Guyana,,GUY,Guyana,
113,IRQ,Iraq,1.0,IRQ,Iraq,,IRQ,Iraq,
114,KUW,Kuwait,1.0,KUW,Kuwait,,KUW,Kuwait,
115,MKD,Macedonia,1.0,MKD,Macedonia,,MKD,Macedonia,
116,MRI,Mauritius,1.0,MRI,Mauritius,,MRI,Mauritius,
117,NIG,Niger,1.0,NIG,Niger,,NIG,Niger,
118,TOG,Togo,1.0,TOG,Togo,,TOG,Togo,
119,PAR,Paraguay,,PAR,Paraguay,17.0,PAR,Paraguay,
120,PER,Peru,,PER,Peru,14.0,PER,Peru,1.0
121,SCG,Serbia,,SCG,Serbia,14.0,SCG,Serbia,
122,NAM,Namibia,,NAM,Namibia,4.0,NAM,Namibia,
123,SIN,Singapore,,SIN,Singapore,4.0,SIN,Singapore,
124,HKG,Hong Kong*,,HKG,Hong Kong*,2.0,HKG,Hong Kong*,1.0
125,SRI,Sri Lanka,,SRI,Sri Lanka,2.0,SRI,Sri Lanka,
126,TAN,Tanzania,,TAN,Tanzania,2.0,TAN,Tanzania,
127,VIE,Vietnam,,VIE,Vietnam,2.0,VIE,Vietnam,
128,ECU,Ecuador,,ECU,Ecuador,1.0,ECU,Ecuador,1.0
129,LUX,Luxembourg,,LUX,Luxembourg,1.0,LUX,Luxembourg,1.0
130,AHO,Netherlands Antilles*,,AHO,Netherlands Antilles*,1.0,AHO,Netherlands Antilles*,
131,CIV,Cote d'Ivoire,,CIV,Cote d'Ivoire,1.0,CIV,Cote d'Ivoire,
132,ISV,Virgin Islands*,,ISV,Virgin Islands*,1.0,ISV,Virgin Islands*,
133,SEN,Senegal,,SEN,Senegal,1.0,SEN,Senegal,
134,SUD,Sudan,,SUD,Sudan,1.0,SUD,Sudan,
135,TGA,Tonga,,TGA,Tonga,1.0,TGA,Tonga,
136,BDI,Burundi,,BDI,Burundi,,BDI,Burundi,1.0
137,UAE,United Arab Emirates,,UAE,United Arab Emirates,,UAE,United Arab Emirates,1.0
\" target=\"_blank\">Download CSV file</a>"
      ],
      "text/plain": [
       "<a download=\"data.csv\" href=\"data:text/csv;base64,,bronze,bronze,bronze,silver,silver,silver,gold,gold,gold
,NOC,Country,Total,NOC,Country,Total,NOC,Country,Total
0,USA,United States,1052.0,USA,United States,1195.0,USA,United States,2088.0
1,URS,Soviet Union,584.0,URS,Soviet Union,627.0,URS,Soviet Union,838.0
2,GBR,United Kingdom,505.0,GBR,United Kingdom,591.0,GBR,United Kingdom,498.0
3,FRA,France,475.0,FRA,France,461.0,FRA,France,378.0
4,GER,Germany,454.0,GER,Germany,350.0,GER,Germany,407.0
5,AUS,Australia,413.0,AUS,Australia,369.0,AUS,Australia,293.0
6,ITA,Italy,374.0,ITA,Italy,394.0,ITA,Italy,460.0
7,HUN,Hungary,345.0,HUN,Hungary,308.0,HUN,Hungary,400.0
8,SWE,Sweden,325.0,SWE,Sweden,349.0,SWE,Sweden,347.0
9,NED,Netherlands,320.0,NED,Netherlands,250.0,NED,Netherlands,212.0
10,ROU,Romania,282.0,ROU,Romania,187.0,ROU,Romania,155.0
11,JPN,Japan,270.0,JPN,Japan,228.0,JPN,Japan,206.0
12,RUS,Russia,240.0,RUS,Russia,206.0,RUS,Russia,192.0
13,CAN,Canada,227.0,CAN,Canada,211.0,CAN,Canada,154.0
14,GDR,East Germany,225.0,GDR,East Germany,271.0,GDR,East Germany,329.0
15,POL,Poland,223.0,POL,Poland,173.0,POL,Poland,103.0
16,FIN,Finland,209.0,FIN,Finland,118.0,FIN,Finland,124.0
17,CHN,China,193.0,CHN,China,252.0,CHN,China,234.0
18,FRG,West Germany,180.0,FRG,West Germany,167.0,FRG,West Germany,143.0
19,BRA,Brazil,174.0,BRA,Brazil,139.0,BRA,Brazil,59.0
20,DEN,Denmark,152.0,DEN,Denmark,192.0,DEN,Denmark,147.0
21,BEL,Belgium,150.0,BEL,Belgium,167.0,BEL,Belgium,91.0
22,NOR,Norway,144.0,NOR,Norway,199.0,NOR,Norway,194.0
23,SUI,Switzerland,138.0,SUI,Switzerland,165.0,SUI,Switzerland,73.0
24,BUL,Bulgaria,136.0,BUL,Bulgaria,142.0,BUL,Bulgaria,53.0
25,KOR,"Korea, South",135.0,KOR,"Korea, South",191.0,KOR,"Korea, South",140.0
26,YUG,Yugoslavia,118.0,YUG,Yugoslavia,174.0,YUG,Yugoslavia,143.0
27,CUB,Cuba,109.0,CUB,Cuba,126.0,CUB,Cuba,160.0
28,TCH,Czechoslovakia,105.0,TCH,Czechoslovakia,144.0,TCH,Czechoslovakia,80.0
29,ESP,Spain,92.0,ESP,Spain,193.0,ESP,Spain,92.0
30,EUA,Unified team of Germany,92.0,EUA,Unified team of Germany,100.0,EUA,Unified team of Germany,68.0
31,ARG,Argentina,88.0,ARG,Argentina,83.0,ARG,Argentina,68.0
32,UKR,Ukraine,78.0,UKR,Ukraine,38.0,UKR,Ukraine,32.0
33,EUN,Unified team,66.0,EUN,Unified team,65.0,EUN,Unified team,92.0
34,NZL,New Zealand,61.0,NZL,New Zealand,27.0,NZL,New Zealand,76.0
35,BLR,Belarus,53.0,BLR,Belarus,25.0,BLR,Belarus,14.0
36,GRE,Greece,49.0,GRE,Greece,62.0,GRE,Greece,34.0
37,MEX,Mexico,48.0,MEX,Mexico,18.0,MEX,Mexico,14.0
38,AUT,Austria,44.0,AUT,Austria,81.0,AUT,Austria,21.0
39,LTU,Lithuania,42.0,LTU,Lithuania,4.0,LTU,Lithuania,4.0
40,JAM,Jamaica,39.0,JAM,Jamaica,40.0,JAM,Jamaica,23.0
41,RSA,South Africa,36.0,RSA,South Africa,37.0,RSA,South Africa,24.0
42,PAK,Pakistan,34.0,PAK,Pakistan,45.0,PAK,Pakistan,42.0
43,IND,India,34.0,IND,India,16.0,IND,India,128.0
44,PRK,"Korea, North",30.0,PRK,"Korea, North",12.0,PRK,"Korea, North",10.0
45,URU,Uruguay,30.0,URU,Uruguay,2.0,URU,Uruguay,44.0
46,NGR,Nigeria,27.0,NGR,Nigeria,38.0,NGR,Nigeria,19.0
47,KEN,Kenya,24.0,KEN,Kenya,31.0,KEN,Kenya,26.0
48,TUR,Turkey,22.0,TUR,Turkey,23.0,TUR,Turkey,37.0
49,IRI,Iran,22.0,IRI,Iran,15.0,IRI,Iran,11.0
50,CHI,Chile,21.0,CHI,Chile,9.0,CHI,Chile,3.0
51,EST,Estonia,19.0,EST,Estonia,9.0,EST,Estonia,9.0
52,CRO,Croatia,18.0,CRO,Croatia,30.0,CRO,Croatia,31.0
53,POR,Portugal,18.0,POR,Portugal,9.0,POR,Portugal,4.0
54,GHA,Ghana,15.0,GHA,Ghana,1.0,GHA,Ghana,
55,KAZ,Kazakhstan,14.0,KAZ,Kazakhstan,16.0,KAZ,Kazakhstan,9.0
56,ETH,Ethiopia,14.0,ETH,Ethiopia,6.0,ETH,Ethiopia,18.0
57,SRB,Serbia,14.0,SRB,Serbia,1.0,SRB,Serbia,
58,TPE,Taiwan,13.0,TPE,Taiwan,27.0,TPE,Taiwan,2.0
59,CZE,Czech Republic,13.0,CZE,Czech Republic,18.0,CZE,Czech Republic,10.0
60,INA,Indonesia,12.0,INA,Indonesia,14.0,INA,Indonesia,9.0
61,TRI,Trinidad and Tobago,11.0,TRI,Trinidad and Tobago,8.0,TRI,Trinidad and Tobago,1.0
62,SLO,Slovenia,11.0,SLO,Slovenia,6.0,SLO,Slovenia,4.0
63,GEO,Georgia,11.0,GEO,Georgia,2.0,GEO,Georgia,5.0
64,ZZX,Mixed teams,10.0,ZZX,Mixed teams,15.0,ZZX,Mixed teams,23.0
65,EGY,Egypt,10.0,EGY,Egypt,7.0,EGY,Egypt,7.0
66,MGL,Mongolia,10.0,MGL,Mongolia,7.0,MGL,Mongolia,2.0
67,MAR,Morocco,10.0,MAR,Morocco,5.0,MAR,Morocco,6.0
68,THA,Thailand,10.0,THA,Thailand,4.0,THA,Thailand,7.0
69,RU1,0,9.0,RU1,0,7.0,RU1,0,1.0
70,AZE,Azerbaijan,9.0,AZE,Azerbaijan,3.0,AZE,Azerbaijan,4.0
71,SVK,Slovakia,8.0,SVK,Slovakia,11.0,SVK,Slovakia,10.0
72,IRL,Ireland,8.0,IRL,Ireland,8.0,IRL,Ireland,8.0
73,UZB,Uzbekistan,8.0,UZB,Uzbekistan,5.0,UZB,Uzbekistan,4.0
74,ALG,Algeria,8.0,ALG,Algeria,2.0,ALG,Algeria,4.0
75,VEN,Venezuela,8.0,VEN,Venezuela,2.0,VEN,Venezuela,1.0
76,COL,Colombia,7.0,COL,Colombia,3.0,COL,Colombia,1.0
77,PHI,Philippines,7.0,PHI,Philippines,2.0,PHI,Philippines,
78,ARM,Armenia,7.0,ARM,Armenia,1.0,ARM,Armenia,1.0
79,HAI,Haiti,7.0,HAI,Haiti,1.0,HAI,Haiti,
80,BOH,Bohemia,6.0,BOH,Bohemia,1.0,BOH,Bohemia,
81,BAH,Bahamas,5.0,BAH,Bahamas,9.0,BAH,Bahamas,9.0
82,ANZ,Australasia,5.0,ANZ,Australasia,4.0,ANZ,Australasia,20.0
83,ISR,Israel,5.0,ISR,Israel,1.0,ISR,Israel,1.0
84,PUR,Puerto Rico*,5.0,PUR,Puerto Rico*,1.0,PUR,Puerto Rico*,
85,BWI,British West Indies,5.0,BWI,British West Indies,,BWI,British West Indies,
86,LAT,Latvia,4.0,LAT,Latvia,11.0,LAT,Latvia,2.0
87,MAS,Malaysia,3.0,MAS,Malaysia,3.0,MAS,Malaysia,
88,MDA,Moldova,3.0,MDA,Moldova,3.0,MDA,Moldova,
89,TUN,Tunisia,3.0,TUN,Tunisia,2.0,TUN,Tunisia,2.0
90,ISL,Iceland,2.0,ISL,Iceland,15.0,ISL,Iceland,
91,UGA,Uganda,2.0,UGA,Uganda,3.0,UGA,Uganda,1.0
92,LIB,Lebanon,2.0,LIB,Lebanon,2.0,LIB,Lebanon,
93,CRC,Costa Rica,2.0,CRC,Costa Rica,1.0,CRC,Costa Rica,1.0
94,IOP,Independent Olympic Participants (1992),2.0,IOP,Independent Olympic Participants (1992),1.0,IOP,Independent Olympic Participants (1992),
95,KGZ,Kyrgyzstan,2.0,KGZ,Kyrgyzstan,1.0,KGZ,Kyrgyzstan,
96,PAN,Panama,2.0,PAN,Panama,,PAN,Panama,1.0
97,QAT,Qatar,2.0,QAT,Qatar,,QAT,Qatar,
98,ZIM,Zimbabwe,1.0,ZIM,Zimbabwe,4.0,ZIM,Zimbabwe,18.0
99,CMR,Cameroon,1.0,CMR,Cameroon,1.0,CMR,Cameroon,20.0
100,DOM,Dominican Republic,1.0,DOM,Dominican Republic,1.0,DOM,Dominican Republic,2.0
101,SYR,Syria,1.0,SYR,Syria,1.0,SYR,Syria,1.0
102,KSA,Saudi Arabia,1.0,KSA,Saudi Arabia,1.0,KSA,Saudi Arabia,
103,TJK,Tajikistan,1.0,TJK,Tajikistan,1.0,TJK,Tajikistan,
104,ZAM,Zambia,1.0,ZAM,Zambia,1.0,ZAM,Zambia,
105,MOZ,Mozambique,1.0,MOZ,Mozambique,,MOZ,Mozambique,1.0
106,SUR,Suriname,1.0,SUR,Suriname,,SUR,Suriname,1.0
107,AFG,Afghanistan,1.0,AFG,Afghanistan,,AFG,Afghanistan,
108,BAR,Barbados,1.0,BAR,Barbados,,BAR,Barbados,
109,BER,Bermuda*,1.0,BER,Bermuda*,,BER,Bermuda*,
110,DJI,Djibouti,1.0,DJI,Djibouti,,DJI,Djibouti,
111,ERI,Eritrea,1.0,ERI,Eritrea,,ERI,Eritrea,
112,GUY,Guyana,1.0,GUY,Guyana,,GUY,Guyana,
113,IRQ,Iraq,1.0,IRQ,Iraq,,IRQ,Iraq,
114,KUW,Kuwait,1.0,KUW,Kuwait,,KUW,Kuwait,
115,MKD,Macedonia,1.0,MKD,Macedonia,,MKD,Macedonia,
116,MRI,Mauritius,1.0,MRI,Mauritius,,MRI,Mauritius,
117,NIG,Niger,1.0,NIG,Niger,,NIG,Niger,
118,TOG,Togo,1.0,TOG,Togo,,TOG,Togo,
119,PAR,Paraguay,,PAR,Paraguay,17.0,PAR,Paraguay,
120,PER,Peru,,PER,Peru,14.0,PER,Peru,1.0
121,SCG,Serbia,,SCG,Serbia,14.0,SCG,Serbia,
122,NAM,Namibia,,NAM,Namibia,4.0,NAM,Namibia,
123,SIN,Singapore,,SIN,Singapore,4.0,SIN,Singapore,
124,HKG,Hong Kong*,,HKG,Hong Kong*,2.0,HKG,Hong Kong*,1.0
125,SRI,Sri Lanka,,SRI,Sri Lanka,2.0,SRI,Sri Lanka,
126,TAN,Tanzania,,TAN,Tanzania,2.0,TAN,Tanzania,
127,VIE,Vietnam,,VIE,Vietnam,2.0,VIE,Vietnam,
128,ECU,Ecuador,,ECU,Ecuador,1.0,ECU,Ecuador,1.0
129,LUX,Luxembourg,,LUX,Luxembourg,1.0,LUX,Luxembourg,1.0
130,AHO,Netherlands Antilles*,,AHO,Netherlands Antilles*,1.0,AHO,Netherlands Antilles*,
131,CIV,Cote d'Ivoire,,CIV,Cote d'Ivoire,1.0,CIV,Cote d'Ivoire,
132,ISV,Virgin Islands*,,ISV,Virgin Islands*,1.0,ISV,Virgin Islands*,
133,SEN,Senegal,,SEN,Senegal,1.0,SEN,Senegal,
134,SUD,Sudan,,SUD,Sudan,1.0,SUD,Sudan,
135,TGA,Tonga,,TGA,Tonga,1.0,TGA,Tonga,
136,BDI,Burundi,,BDI,Burundi,,BDI,Burundi,1.0
137,UAE,United Arab Emirates,,UAE,United Arab Emirates,,UAE,United Arab Emirates,1.0
\" target=\"_blank\">Download CSV file</a>"
      ]
     },
     "execution_count": 155,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "create_download_link(medals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set Index of editions: totals\n",
    "totals = editions.set_index('Edition')\n",
    "\n",
    "# Reassign totals['Grand Total']: totals\n",
    "totals = totals['Grand Total']\n",
    "\n",
    "# Divide medal_counts by totals: fractions\n",
    "fractions = medal_counts.divide(totals,axis='rows')\n",
    "\n",
    "# Print first & last 5 rows of fractions\n",
    "print(fractions.head())\n",
    "print(fractions.tail())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Computing percentage change in fraction of medals won\n",
    "Here, you'll start with the DataFrames editions, medals, medal_counts, & fractions from prior exercises.\n",
    "\n",
    "To see if there is a host country advantage, you first want to see how the fraction of medals won changes from edition to edition.\n",
    "\n",
    "The expanding mean provides a way to see this down each column. It is the value of the mean with all the data available up to that point in time. If you are interested in learning more about pandas' expanding transformations, this section of the pandas documentation has additional information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
